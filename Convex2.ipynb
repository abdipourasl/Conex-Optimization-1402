{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdipourasl/Convex-Optimization-1402/blob/main/Convex2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQPNR08rglpt"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<h1>Convex Optimization Project #2<h1>\n",
        "Amin Abdipour 401133011</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1"
      ],
      "metadata": {
        "id": "zP_NtgJPKmQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define MyGD"
      ],
      "metadata": {
        "id": "eN5F-VOMPcW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyGD(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.001):\n",
        "        defaults = dict(lr=lr)\n",
        "        super(MyGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                p.data.add_(-group['lr'], grad)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "dHZOyDnbKmit"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Optimization Problem <h1>\n",
        "$ \\min f(x_1, x_2) = \\frac{x_2^2}{x_1^2}$ <h1>\n",
        "Subject to $ x_2>0 $"
      ],
      "metadata": {
        "id": "GcvYFbdhQEDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the objective function"
      ],
      "metadata": {
        "id": "rI27zWDFR0VF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(x1, x2):\n",
        "    return x1**2 / x2**2\n",
        "\n",
        "def grad_objective(x1, x2):\n",
        "    grad_x1 = 2 * x1 / x2**2\n",
        "    grad_x2 = -2 * x1**2 / x2**3\n",
        "    return torch.tensor([grad_x1, grad_x2])\n",
        "\n",
        "# Initial values\n",
        "x1 = torch.tensor([1.0], requires_grad=True)\n",
        "z = torch.tensor([0.0], requires_grad=True)"
      ],
      "metadata": {
        "id": "83awsZQjPWqt"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use MyGD"
      ],
      "metadata": {
        "id": "rVa2vxFhR9QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = MyGD([x1, z], lr=0.01)"
      ],
      "metadata": {
        "id": "4-w9MoJXRq96"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GD Loop"
      ],
      "metadata": {
        "id": "SJjxuMlxSOWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    x2 = torch.exp(z)\n",
        "    loss = objective(x1, x2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print or log the iteration information\n",
        "    print(f'Iteration {i + 1}/{1000}, x1: {x1.item()}, x2: {x2.item()}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa3Km9uwSCpR",
        "outputId": "1b2750b2-05ac-4569-dc38-238f109267fa"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1/1000, x1: 0.9800000190734863, x2: 1.0, Loss: 1.0\n",
            "Iteration 2/1000, x1: 0.9611685276031494, x2: 1.020201325416565, Loss: 0.9227423071861267\n",
            "Iteration 3/1000, x1: 0.9433681964874268, x2: 1.0392037630081177, Loss: 0.8554559350013733\n",
            "Iteration 4/1000, x1: 0.9264852404594421, x2: 1.0571366548538208, Loss: 0.7963430881500244\n",
            "Iteration 5/1000, x1: 0.9104242324829102, x2: 1.074108362197876, Loss: 0.7440134882926941\n",
            "Iteration 6/1000, x1: 0.8951044678688049, x2: 1.0902107954025269, Loss: 0.697375476360321\n",
            "Iteration 7/1000, x1: 0.880456805229187, x2: 1.1055231094360352, Loss: 0.6555590629577637\n",
            "Iteration 8/1000, x1: 0.8664217591285706, x2: 1.1201132535934448, Loss: 0.6178630590438843\n",
            "Iteration 9/1000, x1: 0.8529475927352905, x2: 1.1340405941009521, Loss: 0.5837158560752869\n",
            "Iteration 10/1000, x1: 0.8399890661239624, x2: 1.1473573446273804, Loss: 0.552646279335022\n",
            "Iteration 11/1000, x1: 0.827506422996521, x2: 1.1601094007492065, Loss: 0.5242632031440735\n",
            "Iteration 12/1000, x1: 0.8154644966125488, x2: 1.1723374128341675, Loss: 0.49823877215385437\n",
            "Iteration 13/1000, x1: 0.803831934928894, x2: 1.1840778589248657, Loss: 0.47429612278938293\n",
            "Iteration 14/1000, x1: 0.7925808429718018, x2: 1.1953634023666382, Loss: 0.4522000253200531\n",
            "Iteration 15/1000, x1: 0.7816860675811768, x2: 1.2062233686447144, Loss: 0.4317493140697479\n",
            "Iteration 16/1000, x1: 0.7711250185966492, x2: 1.216684103012085, Loss: 0.4127708971500397\n",
            "Iteration 17/1000, x1: 0.760877251625061, x2: 1.2267699241638184, Loss: 0.39511486887931824\n",
            "Iteration 18/1000, x1: 0.7509242296218872, x2: 1.2365026473999023, Loss: 0.3786509931087494\n",
            "Iteration 19/1000, x1: 0.7412490844726562, x2: 1.2459022998809814, Loss: 0.3632655739784241\n",
            "Iteration 20/1000, x1: 0.7318363785743713, x2: 1.254987120628357, Loss: 0.34885892271995544\n",
            "Iteration 21/1000, x1: 0.7226719260215759, x2: 1.263774037361145, Loss: 0.3353429138660431\n",
            "Iteration 22/1000, x1: 0.7137428522109985, x2: 1.2722784280776978, Loss: 0.3226398825645447\n",
            "Iteration 23/1000, x1: 0.7050371766090393, x2: 1.2805147171020508, Loss: 0.3106807768344879\n",
            "Iteration 24/1000, x1: 0.6965438723564148, x2: 1.2884961366653442, Loss: 0.2994041442871094\n",
            "Iteration 25/1000, x1: 0.6882528066635132, x2: 1.2962349653244019, Loss: 0.28875496983528137\n",
            "Iteration 26/1000, x1: 0.6801545023918152, x2: 1.3037424087524414, Loss: 0.27868425846099854\n",
            "Iteration 27/1000, x1: 0.6722401976585388, x2: 1.3110294342041016, Loss: 0.26914745569229126\n",
            "Iteration 28/1000, x1: 0.6645017266273499, x2: 1.3181055784225464, Loss: 0.2601049244403839\n",
            "Iteration 29/1000, x1: 0.6569315195083618, x2: 1.32498037815094, Loss: 0.25152045488357544\n",
            "Iteration 30/1000, x1: 0.6495224833488464, x2: 1.3316624164581299, Loss: 0.24336151778697968\n",
            "Iteration 31/1000, x1: 0.6422679424285889, x2: 1.3381596803665161, Loss: 0.23559848964214325\n",
            "Iteration 32/1000, x1: 0.6351617574691772, x2: 1.3444799184799194, Loss: 0.22820430994033813\n",
            "Iteration 33/1000, x1: 0.6281980276107788, x2: 1.350630283355713, Loss: 0.22115446627140045\n",
            "Iteration 34/1000, x1: 0.621371328830719, x2: 1.3566174507141113, Loss: 0.2144264280796051\n",
            "Iteration 35/1000, x1: 0.6146764755249023, x2: 1.362447738647461, Loss: 0.20799969136714935\n",
            "Iteration 36/1000, x1: 0.608108639717102, x2: 1.3681273460388184, Loss: 0.20185528695583344\n",
            "Iteration 37/1000, x1: 0.6016632318496704, x2: 1.373661756515503, Loss: 0.19597592949867249\n",
            "Iteration 38/1000, x1: 0.5953359007835388, x2: 1.379056453704834, Loss: 0.19034560024738312\n",
            "Iteration 39/1000, x1: 0.5891225934028625, x2: 1.3843164443969727, Loss: 0.184949591755867\n",
            "Iteration 40/1000, x1: 0.583019495010376, x2: 1.389446496963501, Loss: 0.17977432906627655\n",
            "Iteration 41/1000, x1: 0.5770228505134583, x2: 1.3944512605667114, Loss: 0.17480726540088654\n",
            "Iteration 42/1000, x1: 0.5711292624473572, x2: 1.3993350267410278, Loss: 0.1700366735458374\n",
            "Iteration 43/1000, x1: 0.5653353929519653, x2: 1.404101848602295, Loss: 0.16545183956623077\n",
            "Iteration 44/1000, x1: 0.5596381425857544, x2: 1.4087557792663574, Loss: 0.16104266047477722\n",
            "Iteration 45/1000, x1: 0.5540345311164856, x2: 1.4133005142211914, Loss: 0.1567998230457306\n",
            "Iteration 46/1000, x1: 0.5485216975212097, x2: 1.4177396297454834, Loss: 0.15271465480327606\n",
            "Iteration 47/1000, x1: 0.5430969595909119, x2: 1.4220763444900513, Loss: 0.14877904951572418\n",
            "Iteration 48/1000, x1: 0.5377577543258667, x2: 1.4263142347335815, Loss: 0.1449854075908661\n",
            "Iteration 49/1000, x1: 0.5325015783309937, x2: 1.4304560422897339, Loss: 0.14132674038410187\n",
            "Iteration 50/1000, x1: 0.5273261666297913, x2: 1.4345051050186157, Loss: 0.1377963125705719\n",
            "Iteration 51/1000, x1: 0.5222291946411133, x2: 1.4384639263153076, Loss: 0.1343880593776703\n",
            "Iteration 52/1000, x1: 0.5172085762023926, x2: 1.4423353672027588, Loss: 0.13109609484672546\n",
            "Iteration 53/1000, x1: 0.512262225151062, x2: 1.4461220502853394, Loss: 0.12791502475738525\n",
            "Iteration 54/1000, x1: 0.507388174533844, x2: 1.4498263597488403, Loss: 0.12483968585729599\n",
            "Iteration 55/1000, x1: 0.5025845170021057, x2: 1.4534507989883423, Loss: 0.12186528742313385\n",
            "Iteration 56/1000, x1: 0.4978494942188263, x2: 1.4569975137710571, Loss: 0.11898728460073471\n",
            "Iteration 57/1000, x1: 0.49318134784698486, x2: 1.4604690074920654, Loss: 0.11620143055915833\n",
            "Iteration 58/1000, x1: 0.4885784387588501, x2: 1.4638670682907104, Loss: 0.11350368708372116\n",
            "Iteration 59/1000, x1: 0.4840391278266907, x2: 1.4671939611434937, Loss: 0.11089028418064117\n",
            "Iteration 60/1000, x1: 0.4795618951320648, x2: 1.4704514741897583, Loss: 0.10835763812065125\n",
            "Iteration 61/1000, x1: 0.47514528036117554, x2: 1.4736416339874268, Loss: 0.10590234398841858\n",
            "Iteration 62/1000, x1: 0.47078782320022583, x2: 1.4767662286758423, Loss: 0.1035212054848671\n",
            "Iteration 63/1000, x1: 0.46648818254470825, x2: 1.4798269271850586, Loss: 0.10121119767427444\n",
            "Iteration 64/1000, x1: 0.46224501729011536, x2: 1.482825517654419, Loss: 0.0989694595336914\n",
            "Iteration 65/1000, x1: 0.4580570459365845, x2: 1.485763430595398, Loss: 0.0967932641506195\n",
            "Iteration 66/1000, x1: 0.4539230763912201, x2: 1.4886424541473389, Loss: 0.09468001872301102\n",
            "Iteration 67/1000, x1: 0.4498418867588043, x2: 1.4914640188217163, Loss: 0.09262729436159134\n",
            "Iteration 68/1000, x1: 0.4458123445510864, x2: 1.4942295551300049, Loss: 0.09063274413347244\n",
            "Iteration 69/1000, x1: 0.44183334708213806, x2: 1.4969406127929688, Loss: 0.08869416266679764\n",
            "Iteration 70/1000, x1: 0.43790382146835327, x2: 1.4995983839035034, Loss: 0.08680945634841919\n",
            "Iteration 71/1000, x1: 0.4340227544307709, x2: 1.502204179763794, Loss: 0.08497663587331772\n",
            "Iteration 72/1000, x1: 0.4301891326904297, x2: 1.504759430885315, Loss: 0.08319377899169922\n",
            "Iteration 73/1000, x1: 0.4264020025730133, x2: 1.5072652101516724, Loss: 0.08145908266305923\n",
            "Iteration 74/1000, x1: 0.4226604104042053, x2: 1.5097228288650513, Loss: 0.07977081090211868\n",
            "Iteration 75/1000, x1: 0.4189634919166565, x2: 1.5121333599090576, Loss: 0.07812733203172684\n",
            "Iteration 76/1000, x1: 0.4153103232383728, x2: 1.514497995376587, Loss: 0.07652704417705536\n",
            "Iteration 77/1000, x1: 0.4117000997066498, x2: 1.5168178081512451, Loss: 0.07496847212314606\n",
            "Iteration 78/1000, x1: 0.4081319570541382, x2: 1.5190937519073486, Loss: 0.07345016300678253\n",
            "Iteration 79/1000, x1: 0.40460512042045593, x2: 1.5213268995285034, Loss: 0.07197076827287674\n",
            "Iteration 80/1000, x1: 0.40111881494522095, x2: 1.5235183238983154, Loss: 0.07052894681692123\n",
            "Iteration 81/1000, x1: 0.39767229557037354, x2: 1.5256688594818115, Loss: 0.06912346184253693\n",
            "Iteration 82/1000, x1: 0.394264817237854, x2: 1.5277795791625977, Loss: 0.06775311380624771\n",
            "Iteration 83/1000, x1: 0.39089566469192505, x2: 1.5298511981964111, Loss: 0.06641675531864166\n",
            "Iteration 84/1000, x1: 0.38756418228149414, x2: 1.5318846702575684, Loss: 0.06511328369379044\n",
            "Iteration 85/1000, x1: 0.38426968455314636, x2: 1.5338809490203857, Loss: 0.06384162604808807\n",
            "Iteration 86/1000, x1: 0.3810115158557892, x2: 1.535840630531311, Loss: 0.06260081380605698\n",
            "Iteration 87/1000, x1: 0.37778905034065247, x2: 1.5377647876739502, Loss: 0.061389822512865067\n",
            "Iteration 88/1000, x1: 0.37460166215896606, x2: 1.539654016494751, Loss: 0.06020776182413101\n",
            "Iteration 89/1000, x1: 0.3714487850666046, x2: 1.5415091514587402, Loss: 0.05905371159315109\n",
            "Iteration 90/1000, x1: 0.36832982301712036, x2: 1.5433307886123657, Loss: 0.05792684108018875\n",
            "Iteration 91/1000, x1: 0.36524420976638794, x2: 1.5451198816299438, Loss: 0.056826308369636536\n",
            "Iteration 92/1000, x1: 0.362191379070282, x2: 1.5468769073486328, Loss: 0.05575131997466087\n",
            "Iteration 93/1000, x1: 0.3591708242893219, x2: 1.5486027002334595, Loss: 0.05470111966133118\n",
            "Iteration 94/1000, x1: 0.3561820089817047, x2: 1.550297737121582, Loss: 0.05367497354745865\n",
            "Iteration 95/1000, x1: 0.35322439670562744, x2: 1.551962971687317, Loss: 0.05267217382788658\n",
            "Iteration 96/1000, x1: 0.3502975404262543, x2: 1.5535986423492432, Loss: 0.05169204622507095\n",
            "Iteration 97/1000, x1: 0.3474009335041046, x2: 1.5552057027816772, Loss: 0.05073393136262894\n",
            "Iteration 98/1000, x1: 0.34453409910202026, x2: 1.5567845106124878, Loss: 0.04979720339179039\n",
            "Iteration 99/1000, x1: 0.341696560382843, x2: 1.558335781097412, Loss: 0.048881255090236664\n",
            "Iteration 100/1000, x1: 0.33888790011405945, x2: 1.5598599910736084, Loss: 0.0479854941368103\n",
            "Iteration 101/1000, x1: 0.33610767126083374, x2: 1.5613577365875244, Loss: 0.047109365463256836\n",
            "Iteration 102/1000, x1: 0.3333554267883301, x2: 1.562829613685608, Loss: 0.046252328902482986\n",
            "Iteration 103/1000, x1: 0.3306307792663574, x2: 1.564275860786438, Loss: 0.04541385546326637\n",
            "Iteration 104/1000, x1: 0.32793331146240234, x2: 1.5656973123550415, Loss: 0.044593438506126404\n",
            "Iteration 105/1000, x1: 0.3252626061439514, x2: 1.567094326019287, Loss: 0.04379059001803398\n",
            "Iteration 106/1000, x1: 0.322618305683136, x2: 1.568467378616333, Loss: 0.0430048368871212\n",
            "Iteration 107/1000, x1: 0.3199999928474426, x2: 1.569817066192627, Loss: 0.04223572090268135\n",
            "Iteration 108/1000, x1: 0.3174073100090027, x2: 1.571143627166748, Loss: 0.04148280993103981\n",
            "Iteration 109/1000, x1: 0.3148398995399475, x2: 1.572447657585144, Loss: 0.04074566811323166\n",
            "Iteration 110/1000, x1: 0.31229740381240845, x2: 1.5737296342849731, Loss: 0.040023889392614365\n",
            "Iteration 111/1000, x1: 0.30977946519851685, x2: 1.574989914894104, Loss: 0.03931707516312599\n",
            "Iteration 112/1000, x1: 0.30728575587272644, x2: 1.5762288570404053, Loss: 0.0386248417198658\n",
            "Iteration 113/1000, x1: 0.30481594800949097, x2: 1.5774469375610352, Loss: 0.03794681653380394\n",
            "Iteration 114/1000, x1: 0.30236971378326416, x2: 1.5786446332931519, Loss: 0.037282638251781464\n",
            "Iteration 115/1000, x1: 0.29994672536849976, x2: 1.5798221826553345, Loss: 0.03663196042180061\n",
            "Iteration 116/1000, x1: 0.2975466549396515, x2: 1.5809800624847412, Loss: 0.03599444404244423\n",
            "Iteration 117/1000, x1: 0.29516923427581787, x2: 1.5821185111999512, Loss: 0.035369761288166046\n",
            "Iteration 118/1000, x1: 0.29281413555145264, x2: 1.583238124847412, Loss: 0.0347575917840004\n",
            "Iteration 119/1000, x1: 0.2904810905456543, x2: 1.5843391418457031, Loss: 0.034157633781433105\n",
            "Iteration 120/1000, x1: 0.2881697714328766, x2: 1.5854218006134033, Loss: 0.033569592982530594\n",
            "Iteration 121/1000, x1: 0.2858799397945404, x2: 1.586486577987671, Loss: 0.032993171364068985\n",
            "Iteration 122/1000, x1: 0.2836112976074219, x2: 1.5875338315963745, Loss: 0.03242809325456619\n",
            "Iteration 123/1000, x1: 0.2813635766506195, x2: 1.5885636806488037, Loss: 0.03187409043312073\n",
            "Iteration 124/1000, x1: 0.2791365087032318, x2: 1.5895767211914062, Loss: 0.0313308909535408\n",
            "Iteration 125/1000, x1: 0.2769298255443573, x2: 1.5905730724334717, Loss: 0.030798248946666718\n",
            "Iteration 126/1000, x1: 0.27474328875541687, x2: 1.5915530920028687, Loss: 0.03027591109275818\n",
            "Iteration 127/1000, x1: 0.27257663011550903, x2: 1.5925171375274658, Loss: 0.029763633385300636\n",
            "Iteration 128/1000, x1: 0.2704296112060547, x2: 1.5934654474258423, Loss: 0.029261186718940735\n",
            "Iteration 129/1000, x1: 0.26830199360847473, x2: 1.5943982601165771, Loss: 0.02876834198832512\n",
            "Iteration 130/1000, x1: 0.26619356870651245, x2: 1.5953158140182495, Loss: 0.028284888714551926\n",
            "Iteration 131/1000, x1: 0.26410406827926636, x2: 1.5962185859680176, Loss: 0.027810601517558098\n",
            "Iteration 132/1000, x1: 0.26203328371047974, x2: 1.597106695175171, Loss: 0.02734527550637722\n",
            "Iteration 133/1000, x1: 0.2599809765815735, x2: 1.5979803800582886, Loss: 0.026888716965913773\n",
            "Iteration 134/1000, x1: 0.2579469382762909, x2: 1.5988398790359497, Loss: 0.026440715417265892\n",
            "Iteration 135/1000, x1: 0.25593093037605286, x2: 1.5996856689453125, Loss: 0.026001082733273506\n",
            "Iteration 136/1000, x1: 0.25393277406692505, x2: 1.600517749786377, Loss: 0.02556963823735714\n",
            "Iteration 137/1000, x1: 0.25195223093032837, x2: 1.6013363599777222, Loss: 0.025146201252937317\n",
            "Iteration 138/1000, x1: 0.2499891072511673, x2: 1.6021419763565063, Loss: 0.024730587378144264\n",
            "Iteration 139/1000, x1: 0.2480432093143463, x2: 1.6029345989227295, Loss: 0.0243226308375597\n",
            "Iteration 140/1000, x1: 0.2461143434047699, x2: 1.6037145853042603, Loss: 0.023922167718410492\n",
            "Iteration 141/1000, x1: 0.24420230090618134, x2: 1.6044820547103882, Loss: 0.023529034107923508\n",
            "Iteration 142/1000, x1: 0.2423069030046463, x2: 1.6052372455596924, Loss: 0.02314307540655136\n",
            "Iteration 143/1000, x1: 0.24042795598506927, x2: 1.605980396270752, Loss: 0.022764131426811218\n",
            "Iteration 144/1000, x1: 0.23856526613235474, x2: 1.606711745262146, Loss: 0.02239205688238144\n",
            "Iteration 145/1000, x1: 0.23671866953372955, x2: 1.607431411743164, Loss: 0.022026704624295235\n",
            "Iteration 146/1000, x1: 0.2348879724740982, x2: 1.6081397533416748, Loss: 0.021667931228876114\n",
            "Iteration 147/1000, x1: 0.23307301104068756, x2: 1.6088367700576782, Loss: 0.021315602585673332\n",
            "Iteration 148/1000, x1: 0.2312736064195633, x2: 1.609522819519043, Loss: 0.020969577133655548\n",
            "Iteration 149/1000, x1: 0.22948959469795227, x2: 1.610197901725769, Loss: 0.02062973380088806\n",
            "Iteration 150/1000, x1: 0.22772081196308136, x2: 1.6108624935150146, Loss: 0.02029593475162983\n",
            "Iteration 151/1000, x1: 0.22596707940101624, x2: 1.6115164756774902, Loss: 0.01996806263923645\n",
            "Iteration 152/1000, x1: 0.22422824800014496, x2: 1.612160086631775, Loss: 0.019645994529128075\n",
            "Iteration 153/1000, x1: 0.2225041538476944, x2: 1.6127936840057373, Loss: 0.019329605624079704\n",
            "Iteration 154/1000, x1: 0.22079463303089142, x2: 1.6134172677993774, Loss: 0.019018787890672684\n",
            "Iteration 155/1000, x1: 0.21909953653812408, x2: 1.6140310764312744, Loss: 0.01871342398226261\n",
            "Iteration 156/1000, x1: 0.21741871535778046, x2: 1.6146352291107178, Loss: 0.018413404002785683\n",
            "Iteration 157/1000, x1: 0.2157520055770874, x2: 1.6152299642562866, Loss: 0.018118619918823242\n",
            "Iteration 158/1000, x1: 0.21409927308559418, x2: 1.6158154010772705, Loss: 0.017828967422246933\n",
            "Iteration 159/1000, x1: 0.21246036887168884, x2: 1.616391658782959, Loss: 0.017544345930218697\n",
            "Iteration 160/1000, x1: 0.21083515882492065, x2: 1.6169589757919312, Loss: 0.017264652997255325\n",
            "Iteration 161/1000, x1: 0.20922349393367767, x2: 1.617517352104187, Loss: 0.016989797353744507\n",
            "Iteration 162/1000, x1: 0.20762524008750916, x2: 1.6180671453475952, Loss: 0.016719674691557884\n",
            "Iteration 163/1000, x1: 0.20604024827480316, x2: 1.6186082363128662, Loss: 0.016454201191663742\n",
            "Iteration 164/1000, x1: 0.20446839928627014, x2: 1.6191409826278687, Loss: 0.01619327999651432\n",
            "Iteration 165/1000, x1: 0.20290954411029816, x2: 1.619665503501892, Loss: 0.015936823561787605\n",
            "Iteration 166/1000, x1: 0.20136356353759766, x2: 1.6201817989349365, Loss: 0.015684746205806732\n",
            "Iteration 167/1000, x1: 0.1998303234577179, x2: 1.620690107345581, Loss: 0.015436961315572262\n",
            "Iteration 168/1000, x1: 0.19830968976020813, x2: 1.6211905479431152, Loss: 0.015193389728665352\n",
            "Iteration 169/1000, x1: 0.1968015432357788, x2: 1.6216832399368286, Loss: 0.014953946694731712\n",
            "Iteration 170/1000, x1: 0.19530576467514038, x2: 1.6221683025360107, Loss: 0.014718555845320225\n",
            "Iteration 171/1000, x1: 0.1938222348690033, x2: 1.6226459741592407, Loss: 0.014487138018012047\n",
            "Iteration 172/1000, x1: 0.1923508197069168, x2: 1.623116135597229, Loss: 0.014259622432291508\n",
            "Iteration 173/1000, x1: 0.19089141488075256, x2: 1.6235790252685547, Loss: 0.014035931788384914\n",
            "Iteration 174/1000, x1: 0.18944388628005981, x2: 1.6240348815917969, Loss: 0.013815991580486298\n",
            "Iteration 175/1000, x1: 0.1880081295967102, x2: 1.6244837045669556, Loss: 0.01359973568469286\n",
            "Iteration 176/1000, x1: 0.18658402562141418, x2: 1.6249256134033203, Loss: 0.013387092389166355\n",
            "Iteration 177/1000, x1: 0.1851714700460434, x2: 1.6253607273101807, Loss: 0.013177996501326561\n",
            "Iteration 178/1000, x1: 0.1837703436613083, x2: 1.6257892847061157, Loss: 0.012972379103302956\n",
            "Iteration 179/1000, x1: 0.1823805421590805, x2: 1.626211166381836, Loss: 0.012770179659128189\n",
            "Iteration 180/1000, x1: 0.1810019612312317, x2: 1.6266264915466309, Loss: 0.012571332044899464\n",
            "Iteration 181/1000, x1: 0.1796344816684723, x2: 1.6270354986190796, Loss: 0.012375778518617153\n",
            "Iteration 182/1000, x1: 0.17827801406383514, x2: 1.6274383068084717, Loss: 0.01218345109373331\n",
            "Iteration 183/1000, x1: 0.1769324392080307, x2: 1.6278349161148071, Loss: 0.011994298547506332\n",
            "Iteration 184/1000, x1: 0.1755976676940918, x2: 1.6282254457473755, Loss: 0.011808259412646294\n",
            "Iteration 185/1000, x1: 0.17427358031272888, x2: 1.6286098957061768, Loss: 0.011625277809798717\n",
            "Iteration 186/1000, x1: 0.1729600876569748, x2: 1.6289886236190796, Loss: 0.011445295065641403\n",
            "Iteration 187/1000, x1: 0.17165710031986237, x2: 1.629361629486084, Loss: 0.011268259026110172\n",
            "Iteration 188/1000, x1: 0.17036451399326324, x2: 1.62972891330719, Loss: 0.011094118468463421\n",
            "Iteration 189/1000, x1: 0.16908222436904907, x2: 1.6300904750823975, Loss: 0.01092282310128212\n",
            "Iteration 190/1000, x1: 0.16781014204025269, x2: 1.6304466724395752, Loss: 0.010754313319921494\n",
            "Iteration 191/1000, x1: 0.16654817759990692, x2: 1.6307973861694336, Loss: 0.010588548146188259\n",
            "Iteration 192/1000, x1: 0.16529622673988342, x2: 1.6311427354812622, Loss: 0.010425474494695663\n",
            "Iteration 193/1000, x1: 0.16405421495437622, x2: 1.6314828395843506, Loss: 0.010265044867992401\n",
            "Iteration 194/1000, x1: 0.16282203793525696, x2: 1.6318178176879883, Loss: 0.010107213631272316\n",
            "Iteration 195/1000, x1: 0.16159960627555847, x2: 1.6321477890014648, Loss: 0.009951932355761528\n",
            "Iteration 196/1000, x1: 0.1603868305683136, x2: 1.6324726343154907, Loss: 0.009799158200621605\n",
            "Iteration 197/1000, x1: 0.15918363630771637, x2: 1.6327927112579346, Loss: 0.009648844599723816\n",
            "Iteration 198/1000, x1: 0.15798991918563843, x2: 1.6331077814102173, Loss: 0.009500952437520027\n",
            "Iteration 199/1000, x1: 0.1568056046962738, x2: 1.633418083190918, Loss: 0.009355436079204082\n",
            "Iteration 200/1000, x1: 0.15563061833381653, x2: 1.6337237358093262, Loss: 0.009212254546582699\n",
            "Iteration 201/1000, x1: 0.15446485579013824, x2: 1.634024739265442, Loss: 0.009071368724107742\n",
            "Iteration 202/1000, x1: 0.15330825746059418, x2: 1.6343213319778442, Loss: 0.008932734839618206\n",
            "Iteration 203/1000, x1: 0.15216071903705597, x2: 1.634613275527954, Loss: 0.008796320296823978\n",
            "Iteration 204/1000, x1: 0.15102218091487885, x2: 1.6349009275436401, Loss: 0.008662080392241478\n",
            "Iteration 205/1000, x1: 0.14989255368709564, x2: 1.6351841688156128, Loss: 0.008529982529580593\n",
            "Iteration 206/1000, x1: 0.1487717479467392, x2: 1.6354631185531616, Loss: 0.008399986661970615\n",
            "Iteration 207/1000, x1: 0.14765970408916473, x2: 1.6357378959655762, Loss: 0.00827205739915371\n",
            "Iteration 208/1000, x1: 0.1465563327074051, x2: 1.6360085010528564, Loss: 0.008146158419549465\n",
            "Iteration 209/1000, x1: 0.1454615592956543, x2: 1.636275053024292, Loss: 0.008022257126867771\n",
            "Iteration 210/1000, x1: 0.14437532424926758, x2: 1.6365376710891724, Loss: 0.007900316268205643\n",
            "Iteration 211/1000, x1: 0.14329753816127777, x2: 1.636796236038208, Loss: 0.0077803065069019794\n",
            "Iteration 212/1000, x1: 0.1422281265258789, x2: 1.637050986289978, Loss: 0.007662191987037659\n",
            "Iteration 213/1000, x1: 0.1411670297384262, x2: 1.6373018026351929, Loss: 0.007545942440629005\n",
            "Iteration 214/1000, x1: 0.14011415839195251, x2: 1.6375489234924316, Loss: 0.007431525271385908\n",
            "Iteration 215/1000, x1: 0.13906945288181305, x2: 1.6377923488616943, Loss: 0.0073189097456634045\n",
            "Iteration 216/1000, x1: 0.13803283870220184, x2: 1.6380321979522705, Loss: 0.007208063267171383\n",
            "Iteration 217/1000, x1: 0.13700424134731293, x2: 1.6382683515548706, Loss: 0.0070989602245390415\n",
            "Iteration 218/1000, x1: 0.13598360121250153, x2: 1.6385009288787842, Loss: 0.006991568952798843\n",
            "Iteration 219/1000, x1: 0.13497085869312286, x2: 1.6387300491333008, Loss: 0.006885860580950975\n",
            "Iteration 220/1000, x1: 0.13396592438220978, x2: 1.6389557123184204, Loss: 0.006781809963285923\n",
            "Iteration 221/1000, x1: 0.1329687535762787, x2: 1.6391780376434326, Loss: 0.006679384503513575\n",
            "Iteration 222/1000, x1: 0.13197925686836243, x2: 1.6393970251083374, Loss: 0.006578561384230852\n",
            "Iteration 223/1000, x1: 0.13099738955497742, x2: 1.6396126747131348, Loss: 0.006479310803115368\n",
            "Iteration 224/1000, x1: 0.1300230771303177, x2: 1.6398252248764038, Loss: 0.006381608545780182\n",
            "Iteration 225/1000, x1: 0.12905625998973846, x2: 1.640034556388855, Loss: 0.006285428535193205\n",
            "Iteration 226/1000, x1: 0.12809687852859497, x2: 1.6402406692504883, Loss: 0.0061907460913062096\n",
            "Iteration 227/1000, x1: 0.12714485824108124, x2: 1.6404438018798828, Loss: 0.006097536534070969\n",
            "Iteration 228/1000, x1: 0.1262001395225525, x2: 1.6406439542770386, Loss: 0.0060057733207941055\n",
            "Iteration 229/1000, x1: 0.12526267766952515, x2: 1.640841007232666, Loss: 0.005915434565395117\n",
            "Iteration 230/1000, x1: 0.12433239072561264, x2: 1.6410350799560547, Loss: 0.005826498847454786\n",
            "Iteration 231/1000, x1: 0.1234092265367508, x2: 1.6412262916564941, Loss: 0.0057389396242797375\n",
            "Iteration 232/1000, x1: 0.12249313294887543, x2: 1.641414761543274, Loss: 0.0056527345441281796\n",
            "Iteration 233/1000, x1: 0.12158404290676117, x2: 1.641600251197815, Loss: 0.005567864514887333\n",
            "Iteration 234/1000, x1: 0.12068190425634384, x2: 1.6417831182479858, Loss: 0.00548430485650897\n",
            "Iteration 235/1000, x1: 0.11978664994239807, x2: 1.6419631242752075, Loss: 0.005402036011219025\n",
            "Iteration 236/1000, x1: 0.11889822781085968, x2: 1.6421406269073486, Loss: 0.005321035161614418\n",
            "Iteration 237/1000, x1: 0.11801658570766449, x2: 1.64231538772583, Loss: 0.005241283215582371\n",
            "Iteration 238/1000, x1: 0.11714166402816772, x2: 1.6424875259399414, Loss: 0.005162759684026241\n",
            "Iteration 239/1000, x1: 0.1162734106183052, x2: 1.6426571607589722, Loss: 0.005085444543510675\n",
            "Iteration 240/1000, x1: 0.11541176587343216, x2: 1.6428241729736328, Loss: 0.005009318236261606\n",
            "Iteration 241/1000, x1: 0.1145566776394844, x2: 1.6429888010025024, Loss: 0.004934361670166254\n",
            "Iteration 242/1000, x1: 0.11370809376239777, x2: 1.6431509256362915, Loss: 0.004860555287450552\n",
            "Iteration 243/1000, x1: 0.11286595463752747, x2: 1.6433106660842896, Loss: 0.00478788185864687\n",
            "Iteration 244/1000, x1: 0.11203021556138992, x2: 1.6434680223464966, Loss: 0.004716321360319853\n",
            "Iteration 245/1000, x1: 0.11120082437992096, x2: 1.6436231136322021, Loss: 0.004645857494324446\n",
            "Iteration 246/1000, x1: 0.1103777214884758, x2: 1.6437758207321167, Loss: 0.004576472099870443\n",
            "Iteration 247/1000, x1: 0.10956086218357086, x2: 1.6439262628555298, Loss: 0.004508147947490215\n",
            "Iteration 248/1000, x1: 0.10875019431114197, x2: 1.6440744400024414, Loss: 0.004440868739038706\n",
            "Iteration 249/1000, x1: 0.10794566571712494, x2: 1.6442204713821411, Loss: 0.00437461631372571\n",
            "Iteration 250/1000, x1: 0.1071472316980362, x2: 1.644364356994629, Loss: 0.004309375304728746\n",
            "Iteration 251/1000, x1: 0.10635484009981155, x2: 1.6445060968399048, Loss: 0.0042451294139027596\n",
            "Iteration 252/1000, x1: 0.10556843876838684, x2: 1.6446456909179688, Loss: 0.004181863274425268\n",
            "Iteration 253/1000, x1: 0.10478798300027847, x2: 1.6447832584381104, Loss: 0.004119560122489929\n",
            "Iteration 254/1000, x1: 0.10401342809200287, x2: 1.6449187994003296, Loss: 0.004058205522596836\n",
            "Iteration 255/1000, x1: 0.10324472188949585, x2: 1.6450523138046265, Loss: 0.003997785039246082\n",
            "Iteration 256/1000, x1: 0.10248181968927383, x2: 1.645183801651001, Loss: 0.003938282839953899\n",
            "Iteration 257/1000, x1: 0.10172467678785324, x2: 1.6453135013580322, Loss: 0.0038796840235590935\n",
            "Iteration 258/1000, x1: 0.10097324103116989, x2: 1.6454411745071411, Loss: 0.0038219757843762636\n",
            "Iteration 259/1000, x1: 0.1002274677157402, x2: 1.6455669403076172, Loss: 0.0037651429884135723\n",
            "Iteration 260/1000, x1: 0.0994873195886612, x2: 1.64569091796875, Loss: 0.003709171898663044\n",
            "Iteration 261/1000, x1: 0.09875274449586868, x2: 1.64581298828125, Loss: 0.0036540499422699213\n",
            "Iteration 262/1000, x1: 0.09802369773387909, x2: 1.6459332704544067, Loss: 0.0035997629165649414\n",
            "Iteration 263/1000, x1: 0.09730014204978943, x2: 1.6460517644882202, Loss: 0.003546297550201416\n",
            "Iteration 264/1000, x1: 0.09658202528953552, x2: 1.6461684703826904, Loss: 0.003493641968816519\n",
            "Iteration 265/1000, x1: 0.09586931020021439, x2: 1.646283507347107, Loss: 0.003441781969740987\n",
            "Iteration 266/1000, x1: 0.09516195207834244, x2: 1.6463968753814697, Loss: 0.0033907059114426374\n",
            "Iteration 267/1000, x1: 0.0944599062204361, x2: 1.6465084552764893, Loss: 0.0033404019195586443\n",
            "Iteration 268/1000, x1: 0.09376313537359238, x2: 1.6466184854507446, Loss: 0.00329085742123425\n",
            "Iteration 269/1000, x1: 0.0930715948343277, x2: 1.6467268466949463, Loss: 0.0032420605421066284\n",
            "Iteration 270/1000, x1: 0.09238523989915848, x2: 1.6468336582183838, Loss: 0.003193999407812953\n",
            "Iteration 271/1000, x1: 0.09170403331518173, x2: 1.6469388008117676, Loss: 0.003146663075312972\n",
            "Iteration 272/1000, x1: 0.09102793782949448, x2: 1.6470425128936768, Loss: 0.0031000396702438593\n",
            "Iteration 273/1000, x1: 0.09035690873861313, x2: 1.6471446752548218, Loss: 0.0030541187152266502\n",
            "Iteration 274/1000, x1: 0.0896909087896347, x2: 1.6472452878952026, Loss: 0.003008889267221093\n",
            "Iteration 275/1000, x1: 0.08902989327907562, x2: 1.6473443508148193, Loss: 0.0029643401503562927\n",
            "Iteration 276/1000, x1: 0.0883738324046135, x2: 1.6474419832229614, Loss: 0.002920461120083928\n",
            "Iteration 277/1000, x1: 0.08772268146276474, x2: 1.6475383043289185, Loss: 0.002877241699025035\n",
            "Iteration 278/1000, x1: 0.08707640320062637, x2: 1.6476330757141113, Loss: 0.002834671875461936\n",
            "Iteration 279/1000, x1: 0.08643496036529541, x2: 1.6477264165878296, Loss: 0.0027927416376769543\n",
            "Iteration 280/1000, x1: 0.08579830825328827, x2: 1.6478184461593628, Loss: 0.002751440741121769\n",
            "Iteration 281/1000, x1: 0.08516641706228256, x2: 1.647909164428711, Loss: 0.002710759174078703\n",
            "Iteration 282/1000, x1: 0.0845392495393753, x2: 1.6479984521865845, Loss: 0.0026706878561526537\n",
            "Iteration 283/1000, x1: 0.08391676843166351, x2: 1.6480865478515625, Loss: 0.002631217474117875\n",
            "Iteration 284/1000, x1: 0.0832989290356636, x2: 1.648173213005066, Loss: 0.0025923389475792646\n",
            "Iteration 285/1000, x1: 0.08268570899963379, x2: 1.6482586860656738, Loss: 0.002554042497649789\n",
            "Iteration 286/1000, x1: 0.08207706362009048, x2: 1.6483429670333862, Loss: 0.002516319276764989\n",
            "Iteration 287/1000, x1: 0.0814729556441307, x2: 1.6484259366989136, Loss: 0.002479161135852337\n",
            "Iteration 288/1000, x1: 0.08087335526943207, x2: 1.6485075950622559, Loss: 0.0024425589945167303\n",
            "Iteration 289/1000, x1: 0.08027822524309158, x2: 1.6485881805419922, Loss: 0.0024065037723630667\n",
            "Iteration 290/1000, x1: 0.07968753576278687, x2: 1.648667573928833, Loss: 0.0023709877859801054\n",
            "Iteration 291/1000, x1: 0.07910124212503433, x2: 1.6487457752227783, Loss: 0.0023360031191259623\n",
            "Iteration 292/1000, x1: 0.0785193219780922, x2: 1.6488227844238281, Loss: 0.0023015406914055347\n",
            "Iteration 293/1000, x1: 0.07794173061847687, x2: 1.6488986015319824, Loss: 0.0022675932850688696\n",
            "Iteration 294/1000, x1: 0.07736844569444656, x2: 1.6489734649658203, Loss: 0.002234152052551508\n",
            "Iteration 295/1000, x1: 0.0767994225025177, x2: 1.6490471363067627, Loss: 0.002201210707426071\n",
            "Iteration 296/1000, x1: 0.07623463869094849, x2: 1.6491198539733887, Loss: 0.0021687597036361694\n",
            "Iteration 297/1000, x1: 0.07567405700683594, x2: 1.6491913795471191, Loss: 0.0021367936860769987\n",
            "Iteration 298/1000, x1: 0.07511764019727707, x2: 1.6492618322372437, Loss: 0.002105304040014744\n",
            "Iteration 299/1000, x1: 0.07456536591053009, x2: 1.6493312120437622, Loss: 0.0020742835476994514\n",
            "Iteration 300/1000, x1: 0.07401719689369202, x2: 1.6493996381759644, Loss: 0.0020437254570424557\n",
            "Iteration 301/1000, x1: 0.07347310334444046, x2: 1.64946711063385, Loss: 0.002013622084632516\n",
            "Iteration 302/1000, x1: 0.07293304800987244, x2: 1.6495336294174194, Loss: 0.00198396691121161\n",
            "Iteration 303/1000, x1: 0.07239700853824615, x2: 1.6495990753173828, Loss: 0.0019547531846910715\n",
            "Iteration 304/1000, x1: 0.07186494767665863, x2: 1.6496635675430298, Loss: 0.0019259743858128786\n",
            "Iteration 305/1000, x1: 0.07133684307336807, x2: 1.6497271060943604, Loss: 0.0018976232968270779\n",
            "Iteration 306/1000, x1: 0.0708126574754715, x2: 1.6497896909713745, Loss: 0.0018696943297982216\n",
            "Iteration 307/1000, x1: 0.07029236108064651, x2: 1.6498514413833618, Loss: 0.0018421801505610347\n",
            "Iteration 308/1000, x1: 0.06977592408657074, x2: 1.6499121189117432, Loss: 0.0018150752875953913\n",
            "Iteration 309/1000, x1: 0.06926331669092178, x2: 1.6499720811843872, Loss: 0.00178837263956666\n",
            "Iteration 310/1000, x1: 0.06875451654195786, x2: 1.6500310897827148, Loss: 0.0017620665021240711\n",
            "Iteration 311/1000, x1: 0.06824948638677597, x2: 1.650089144706726, Loss: 0.0017361515201628208\n",
            "Iteration 312/1000, x1: 0.06774820387363434, x2: 1.650146484375, Loss: 0.0017106208251789212\n",
            "Iteration 313/1000, x1: 0.06725063174962997, x2: 1.650202989578247, Loss: 0.0016854694113135338\n",
            "Iteration 314/1000, x1: 0.06675674766302109, x2: 1.6502586603164673, Loss: 0.0016606904100626707\n",
            "Iteration 315/1000, x1: 0.06626652926206589, x2: 1.6503134965896606, Loss: 0.0016362792812287807\n",
            "Iteration 316/1000, x1: 0.06577993929386139, x2: 1.6503674983978271, Loss: 0.0016122304368764162\n",
            "Iteration 317/1000, x1: 0.0652969554066658, x2: 1.6504206657409668, Loss: 0.0015885381726548076\n",
            "Iteration 318/1000, x1: 0.06481754779815674, x2: 1.6504731178283691, Loss: 0.0015651969006285071\n",
            "Iteration 319/1000, x1: 0.06434168666601181, x2: 1.6505247354507446, Loss: 0.0015422013821080327\n",
            "Iteration 320/1000, x1: 0.06386934965848923, x2: 1.6505756378173828, Loss: 0.0015195466112345457\n",
            "Iteration 321/1000, x1: 0.0634005069732666, x2: 1.6506258249282837, Loss: 0.001497227349318564\n",
            "Iteration 322/1000, x1: 0.06293513625860214, x2: 1.6506751775741577, Loss: 0.0014752383576706052\n",
            "Iteration 323/1000, x1: 0.06247320771217346, x2: 1.650723934173584, Loss: 0.0014535750960931182\n",
            "Iteration 324/1000, x1: 0.06201469525694847, x2: 1.6507718563079834, Loss: 0.0014322323258966208\n",
            "Iteration 325/1000, x1: 0.06155957654118538, x2: 1.650819182395935, Loss: 0.0014112053904682398\n",
            "Iteration 326/1000, x1: 0.0611078217625618, x2: 1.650865912437439, Loss: 0.0013904892839491367\n",
            "Iteration 327/1000, x1: 0.06065940856933594, x2: 1.650911808013916, Loss: 0.001370079698972404\n",
            "Iteration 328/1000, x1: 0.06021431088447571, x2: 1.6509571075439453, Loss: 0.0013499719789251685\n",
            "Iteration 329/1000, x1: 0.05977250263094902, x2: 1.6510016918182373, Loss: 0.0013301617000252008\n",
            "Iteration 330/1000, x1: 0.059333957731723785, x2: 1.651045560836792, Loss: 0.001310644089244306\n",
            "Iteration 331/1000, x1: 0.05889865383505821, x2: 1.651088833808899, Loss: 0.0012914148392155766\n",
            "Iteration 332/1000, x1: 0.05846656486392021, x2: 1.651131510734558, Loss: 0.001272469642572105\n",
            "Iteration 333/1000, x1: 0.05803766846656799, x2: 1.65117347240448, Loss: 0.0012538041919469833\n",
            "Iteration 334/1000, x1: 0.05761193856596947, x2: 1.6512149572372437, Loss: 0.0012354145292192698\n",
            "Iteration 335/1000, x1: 0.05718935281038284, x2: 1.65125572681427, Loss: 0.0012172962306067348\n",
            "Iteration 336/1000, x1: 0.05676988884806633, x2: 1.6512959003448486, Loss: 0.0011994455708190799\n",
            "Iteration 337/1000, x1: 0.05635352060198784, x2: 1.6513354778289795, Loss: 0.0011818584753200412\n",
            "Iteration 338/1000, x1: 0.05594022572040558, x2: 1.6513745784759521, Loss: 0.001164530636742711\n",
            "Iteration 339/1000, x1: 0.05552998185157776, x2: 1.651413083076477, Loss: 0.001147458446212113\n",
            "Iteration 340/1000, x1: 0.05512276291847229, x2: 1.6514509916305542, Loss: 0.0011306381784379482\n",
            "Iteration 341/1000, x1: 0.05471855029463768, x2: 1.6514883041381836, Loss: 0.0011140661081299186\n",
            "Iteration 342/1000, x1: 0.05431731790304184, x2: 1.6515250205993652, Loss: 0.0010977382771670818\n",
            "Iteration 343/1000, x1: 0.053919047117233276, x2: 1.6515612602233887, Loss: 0.0010816511930897832\n",
            "Iteration 344/1000, x1: 0.0535237118601799, x2: 1.651597023010254, Loss: 0.001065801247023046\n",
            "Iteration 345/1000, x1: 0.05313129350543022, x2: 1.651632308959961, Loss: 0.0010501848300918937\n",
            "Iteration 346/1000, x1: 0.05274176970124245, x2: 1.6516668796539307, Loss: 0.001034798682667315\n",
            "Iteration 347/1000, x1: 0.052355118095874786, x2: 1.6517010927200317, Loss: 0.0010196390794590116\n",
            "Iteration 348/1000, x1: 0.05197131633758545, x2: 1.651734709739685, Loss: 0.0010047029936686158\n",
            "Iteration 349/1000, x1: 0.051590342074632645, x2: 1.6517679691314697, Loss: 0.0009899867000058293\n",
            "Iteration 350/1000, x1: 0.05121217668056488, x2: 1.6518006324768066, Loss: 0.0009754871716722846\n",
            "Iteration 351/1000, x1: 0.050836797803640366, x2: 1.6518328189849854, Loss: 0.0009612011490389705\n",
            "Iteration 352/1000, x1: 0.05046418309211731, x2: 1.6518646478652954, Loss: 0.0009471253724768758\n",
            "Iteration 353/1000, x1: 0.05009431391954422, x2: 1.6518958806991577, Loss: 0.000933256815187633\n",
            "Iteration 354/1000, x1: 0.049727167934179306, x2: 1.6519267559051514, Loss: 0.0009195922175422311\n",
            "Iteration 355/1000, x1: 0.049362726509571075, x2: 1.6519571542739868, Loss: 0.0009061286691576242\n",
            "Iteration 356/1000, x1: 0.049000971019268036, x2: 1.651987075805664, Loss: 0.0008928633178584278\n",
            "Iteration 357/1000, x1: 0.0486418791115284, x2: 1.6520166397094727, Loss: 0.0008797930786386132\n",
            "Iteration 358/1000, x1: 0.048285432159900665, x2: 1.652045726776123, Loss: 0.000866915041115135\n",
            "Iteration 359/1000, x1: 0.04793160781264305, x2: 1.6520743370056152, Loss: 0.0008542265859432518\n",
            "Iteration 360/1000, x1: 0.04758038744330406, x2: 1.6521025896072388, Loss: 0.0008417244534939528\n",
            "Iteration 361/1000, x1: 0.047231752425432205, x2: 1.652130365371704, Loss: 0.000829406373668462\n",
            "Iteration 362/1000, x1: 0.04688568413257599, x2: 1.6521577835083008, Loss: 0.0008172691450454295\n",
            "Iteration 363/1000, x1: 0.04654216393828392, x2: 1.6521847248077393, Loss: 0.0008053103811107576\n",
            "Iteration 364/1000, x1: 0.04620116949081421, x2: 1.652211308479309, Loss: 0.0007935274625197053\n",
            "Iteration 365/1000, x1: 0.04586268588900566, x2: 1.6522375345230103, Loss: 0.0007819175370968878\n",
            "Iteration 366/1000, x1: 0.045526690781116486, x2: 1.6522632837295532, Loss: 0.0007704783929511905\n",
            "Iteration 367/1000, x1: 0.04519316926598549, x2: 1.652288794517517, Loss: 0.0007592071196995676\n",
            "Iteration 368/1000, x1: 0.04486210271716118, x2: 1.6523139476776123, Loss: 0.0007481013890355825\n",
            "Iteration 369/1000, x1: 0.044533468782901764, x2: 1.6523386240005493, Loss: 0.0007371589890681207\n",
            "Iteration 370/1000, x1: 0.04420725256204605, x2: 1.6523629426956177, Loss: 0.0007263770676217973\n",
            "Iteration 371/1000, x1: 0.04388343542814255, x2: 1.652387022972107, Loss: 0.0007157535292208195\n",
            "Iteration 372/1000, x1: 0.04356199875473976, x2: 1.652410626411438, Loss: 0.0007052859873510897\n",
            "Iteration 373/1000, x1: 0.0432429276406765, x2: 1.65243399143219, Loss: 0.0006949720554985106\n",
            "Iteration 374/1000, x1: 0.04292619973421097, x2: 1.6524569988250732, Loss: 0.0006848095799796283\n",
            "Iteration 375/1000, x1: 0.042611800134181976, x2: 1.652479648590088, Loss: 0.0006747962324880064\n",
            "Iteration 376/1000, x1: 0.04229971393942833, x2: 1.6525018215179443, Loss: 0.0006649299175478518\n",
            "Iteration 377/1000, x1: 0.041989922523498535, x2: 1.6525238752365112, Loss: 0.0006552082486450672\n",
            "Iteration 378/1000, x1: 0.0416824072599411, x2: 1.65254545211792, Loss: 0.0006456294213421643\n",
            "Iteration 379/1000, x1: 0.041377149522304535, x2: 1.652566909790039, Loss: 0.0006361909909173846\n",
            "Iteration 380/1000, x1: 0.04107413813471794, x2: 1.652587890625, Loss: 0.0006268909783102572\n",
            "Iteration 381/1000, x1: 0.04077335074543953, x2: 1.6526085138320923, Loss: 0.0006177275208756328\n",
            "Iteration 382/1000, x1: 0.040474772453308105, x2: 1.6526288986206055, Loss: 0.0006086982903070748\n",
            "Iteration 383/1000, x1: 0.040178388357162476, x2: 1.6526490449905396, Loss: 0.0005998015403747559\n",
            "Iteration 384/1000, x1: 0.039884183555841446, x2: 1.652668833732605, Loss: 0.000591035233810544\n",
            "Iteration 385/1000, x1: 0.039592139422893524, x2: 1.6526883840560913, Loss: 0.0005823974497616291\n",
            "Iteration 386/1000, x1: 0.039302241057157516, x2: 1.652707576751709, Loss: 0.0005738863255828619\n",
            "Iteration 387/1000, x1: 0.03901446983218193, x2: 1.6527265310287476, Loss: 0.0005655000568367541\n",
            "Iteration 388/1000, x1: 0.03872881457209587, x2: 1.652745246887207, Loss: 0.0005572364898398519\n",
            "Iteration 389/1000, x1: 0.038445256650447845, x2: 1.6527637243270874, Loss: 0.0005490942276082933\n",
            "Iteration 390/1000, x1: 0.03816378116607666, x2: 1.6527818441390991, Loss: 0.0005410712910816073\n",
            "Iteration 391/1000, x1: 0.03788437321782112, x2: 1.6527997255325317, Loss: 0.0005331658758223057\n",
            "Iteration 392/1000, x1: 0.037607014179229736, x2: 1.6528173685073853, Loss: 0.0005253762938082218\n",
            "Iteration 393/1000, x1: 0.03733169287443161, x2: 1.6528347730636597, Loss: 0.0005177007988095284\n",
            "Iteration 394/1000, x1: 0.03705839440226555, x2: 1.6528518199920654, Loss: 0.0005101378774270415\n",
            "Iteration 395/1000, x1: 0.03678710013628006, x2: 1.6528687477111816, Loss: 0.0005026856088079512\n",
            "Iteration 396/1000, x1: 0.03651779890060425, x2: 1.6528853178024292, Loss: 0.0004953425959683955\n",
            "Iteration 397/1000, x1: 0.03625047206878662, x2: 1.6529017686843872, Loss: 0.00048810706357471645\n",
            "Iteration 398/1000, x1: 0.035985108464956284, x2: 1.6529178619384766, Loss: 0.00048097752733156085\n",
            "Iteration 399/1000, x1: 0.03572169318795204, x2: 1.6529337167739868, Loss: 0.000473952415632084\n",
            "Iteration 400/1000, x1: 0.0354602113366127, x2: 1.6529494524002075, Loss: 0.00046703018597327173\n",
            "Iteration 401/1000, x1: 0.03520064800977707, x2: 1.6529648303985596, Loss: 0.00046020932495594025\n",
            "Iteration 402/1000, x1: 0.03494298830628395, x2: 1.652980089187622, Loss: 0.0004534882609732449\n",
            "Iteration 403/1000, x1: 0.03468722105026245, x2: 1.652994990348816, Loss: 0.00044686568435281515\n",
            "Iteration 404/1000, x1: 0.03443332761526108, x2: 1.6530097723007202, Loss: 0.0004403400234878063\n",
            "Iteration 405/1000, x1: 0.034181296825408936, x2: 1.653024435043335, Loss: 0.0004339097940828651\n",
            "Iteration 406/1000, x1: 0.03393111750483513, x2: 1.653038740158081, Loss: 0.00042757371556945145\n",
            "Iteration 407/1000, x1: 0.033682774752378464, x2: 1.653052806854248, Loss: 0.00042133047827519476\n",
            "Iteration 408/1000, x1: 0.03343625366687775, x2: 1.6530667543411255, Loss: 0.00041517859790474176\n",
            "Iteration 409/1000, x1: 0.03319153934717178, x2: 1.6530804634094238, Loss: 0.000409116706578061\n",
            "Iteration 410/1000, x1: 0.03294862061738968, x2: 1.653093934059143, Loss: 0.00040314352372661233\n",
            "Iteration 411/1000, x1: 0.03270748257637024, x2: 1.6531072854995728, Loss: 0.0003972577687818557\n",
            "Iteration 412/1000, x1: 0.03246811404824257, x2: 1.6531203985214233, Loss: 0.00039145807386375964\n",
            "Iteration 413/1000, x1: 0.032230500131845474, x2: 1.6531332731246948, Loss: 0.0003857433039229363\n",
            "Iteration 414/1000, x1: 0.03199462965130806, x2: 1.6531460285186768, Loss: 0.00038011206197552383\n",
            "Iteration 415/1000, x1: 0.03176048770546913, x2: 1.6531585454940796, Loss: 0.0003745632420759648\n",
            "Iteration 416/1000, x1: 0.031528063118457794, x2: 1.6531709432601929, Loss: 0.00036909556365571916\n",
            "Iteration 417/1000, x1: 0.03129734471440315, x2: 1.6531832218170166, Loss: 0.00036370777525007725\n",
            "Iteration 418/1000, x1: 0.031068315729498863, x2: 1.6531952619552612, Loss: 0.00035839888732880354\n",
            "Iteration 419/1000, x1: 0.03084096685051918, x2: 1.6532070636749268, Loss: 0.0003531676484271884\n",
            "Iteration 420/1000, x1: 0.03061528503894806, x2: 1.6532187461853027, Loss: 0.00034801289439201355\n",
            "Iteration 421/1000, x1: 0.030391257256269455, x2: 1.6532303094863892, Loss: 0.0003429334901738912\n",
            "Iteration 422/1000, x1: 0.030168872326612473, x2: 1.6532416343688965, Loss: 0.0003379283589310944\n",
            "Iteration 423/1000, x1: 0.029948117211461067, x2: 1.6532528400421143, Loss: 0.00033299645292572677\n",
            "Iteration 424/1000, x1: 0.029728980734944344, x2: 1.653263807296753, Loss: 0.00032813663710840046\n",
            "Iteration 425/1000, x1: 0.029511449858546257, x2: 1.653274655342102, Loss: 0.0003233478928450495\n",
            "Iteration 426/1000, x1: 0.029295513406395912, x2: 1.653285264968872, Loss: 0.000318629143293947\n",
            "Iteration 427/1000, x1: 0.029081160202622414, x2: 1.653295874595642, Loss: 0.00031397934071719646\n",
            "Iteration 428/1000, x1: 0.028868377208709717, x2: 1.653306245803833, Loss: 0.0003093975246883929\n",
            "Iteration 429/1000, x1: 0.028657155111432076, x2: 1.6533164978027344, Loss: 0.00030488267657347023\n",
            "Iteration 430/1000, x1: 0.028447480872273445, x2: 1.6533265113830566, Loss: 0.0003004338650498539\n",
            "Iteration 431/1000, x1: 0.02823934331536293, x2: 1.6533364057540894, Loss: 0.00029605007148347795\n",
            "Iteration 432/1000, x1: 0.028032731264829636, x2: 1.6533461809158325, Loss: 0.00029173033544793725\n",
            "Iteration 433/1000, x1: 0.027827633544802666, x2: 1.6533558368682861, Loss: 0.0002874737256206572\n",
            "Iteration 434/1000, x1: 0.027624037116765976, x2: 1.6533653736114502, Loss: 0.00028327933978289366\n",
            "Iteration 435/1000, x1: 0.02742193266749382, x2: 1.6533746719360352, Loss: 0.00027914621750824153\n",
            "Iteration 436/1000, x1: 0.027221309021115303, x2: 1.6533839702606201, Loss: 0.0002750734565779567\n",
            "Iteration 437/1000, x1: 0.02702215686440468, x2: 1.653393030166626, Loss: 0.00027106021298095584\n",
            "Iteration 438/1000, x1: 0.0268244631588459, x2: 1.6534019708633423, Loss: 0.00026710567180998623\n",
            "Iteration 439/1000, x1: 0.026628218591213226, x2: 1.6534109115600586, Loss: 0.00026320884353481233\n",
            "Iteration 440/1000, x1: 0.026433410122990608, x2: 1.6534194946289062, Loss: 0.00025936902966350317\n",
            "Iteration 441/1000, x1: 0.02624003030359745, x2: 1.653428077697754, Loss: 0.0002555852406658232\n",
            "Iteration 442/1000, x1: 0.026048066094517708, x2: 1.653436541557312, Loss: 0.00025185674894601107\n",
            "Iteration 443/1000, x1: 0.025857508182525635, x2: 1.6534450054168701, Loss: 0.000248182681389153\n",
            "Iteration 444/1000, x1: 0.025668347254395485, x2: 1.6534531116485596, Loss: 0.00024456233950331807\n",
            "Iteration 445/1000, x1: 0.025480572134256363, x2: 1.653461217880249, Loss: 0.0002409948647255078\n",
            "Iteration 446/1000, x1: 0.025294171646237373, x2: 1.653469204902649, Loss: 0.0002374795003561303\n",
            "Iteration 447/1000, x1: 0.02510913647711277, x2: 1.6534770727157593, Loss: 0.00023401546059176326\n",
            "Iteration 448/1000, x1: 0.024925457313656807, x2: 1.65348482131958, Loss: 0.00023060201783664525\n",
            "Iteration 449/1000, x1: 0.02474312297999859, x2: 1.6534924507141113, Loss: 0.00022723844449501485\n",
            "Iteration 450/1000, x1: 0.02456212416291237, x2: 1.653499960899353, Loss: 0.00022392399841919541\n",
            "Iteration 451/1000, x1: 0.0243824515491724, x2: 1.6535073518753052, Loss: 0.00022065796656534076\n",
            "Iteration 452/1000, x1: 0.02420409396290779, x2: 1.6535146236419678, Loss: 0.00021743960678577423\n",
            "Iteration 453/1000, x1: 0.024027042090892792, x2: 1.6535217761993408, Loss: 0.00021426824969239533\n",
            "Iteration 454/1000, x1: 0.023851286619901657, x2: 1.6535289287567139, Loss: 0.00021114316768944263\n",
            "Iteration 455/1000, x1: 0.02367681823670864, x2: 1.6535359621047974, Loss: 0.0002080637204926461\n",
            "Iteration 456/1000, x1: 0.023503627628087997, x2: 1.6535428762435913, Loss: 0.00020502920961007476\n",
            "Iteration 457/1000, x1: 0.02333170548081398, x2: 1.6535496711730957, Loss: 0.00020203903841320425\n",
            "Iteration 458/1000, x1: 0.023161042481660843, x2: 1.6535563468933105, Loss: 0.00019909252296201885\n",
            "Iteration 459/1000, x1: 0.02299162931740284, x2: 1.6535629034042358, Loss: 0.00019618905207607895\n",
            "Iteration 460/1000, x1: 0.022823456674814224, x2: 1.6535694599151611, Loss: 0.0001933279272634536\n",
            "Iteration 461/1000, x1: 0.02265651524066925, x2: 1.6535758972167969, Loss: 0.00019050859555136412\n",
            "Iteration 462/1000, x1: 0.022490795701742172, x2: 1.6535820960998535, Loss: 0.00018773044575937092\n",
            "Iteration 463/1000, x1: 0.022326290607452393, x2: 1.6535884141921997, Loss: 0.00018499277939554304\n",
            "Iteration 464/1000, x1: 0.022162990644574165, x2: 1.6535944938659668, Loss: 0.0001822951453505084\n",
            "Iteration 465/1000, x1: 0.022000884637236595, x2: 1.6536004543304443, Loss: 0.00017963690333999693\n",
            "Iteration 466/1000, x1: 0.021839966997504234, x2: 1.6536064147949219, Loss: 0.00017701741307973862\n",
            "Iteration 467/1000, x1: 0.021680226549506187, x2: 1.6536122560501099, Loss: 0.0001744361943565309\n",
            "Iteration 468/1000, x1: 0.021521655842661858, x2: 1.6536179780960083, Loss: 0.00017189262143801898\n",
            "Iteration 469/1000, x1: 0.0213642455637455, x2: 1.6536237001419067, Loss: 0.00016938618500716984\n",
            "Iteration 470/1000, x1: 0.021207988262176514, x2: 1.6536293029785156, Loss: 0.0001669163175392896\n",
            "Iteration 471/1000, x1: 0.021052874624729156, x2: 1.6536349058151245, Loss: 0.00016448249516543\n",
            "Iteration 472/1000, x1: 0.02089889720082283, x2: 1.6536402702331543, Loss: 0.00016208422312047333\n",
            "Iteration 473/1000, x1: 0.02074604667723179, x2: 1.653645634651184, Loss: 0.00015972093387972564\n",
            "Iteration 474/1000, x1: 0.020594313740730286, x2: 1.6536509990692139, Loss: 0.00015739211812615395\n",
            "Iteration 475/1000, x1: 0.020443692803382874, x2: 1.6536561250686646, Loss: 0.00015509729564655572\n",
            "Iteration 476/1000, x1: 0.020294174551963806, x2: 1.6536612510681152, Loss: 0.0001528359716758132\n",
            "Iteration 477/1000, x1: 0.020145749673247337, x2: 1.653666377067566, Loss: 0.00015060763689689338\n",
            "Iteration 478/1000, x1: 0.01999841071665287, x2: 1.653671383857727, Loss: 0.0001484117965446785\n",
            "Iteration 479/1000, x1: 0.019852150231599808, x2: 1.6536762714385986, Loss: 0.00014624801406171173\n",
            "Iteration 480/1000, x1: 0.019706960767507553, x2: 1.6536811590194702, Loss: 0.00014411578013096005\n",
            "Iteration 481/1000, x1: 0.01956283487379551, x2: 1.6536858081817627, Loss: 0.0001420147018507123\n",
            "Iteration 482/1000, x1: 0.01941976323723793, x2: 1.6536905765533447, Loss: 0.00013994425535202026\n",
            "Iteration 483/1000, x1: 0.01927773840725422, x2: 1.6536952257156372, Loss: 0.00013790401862934232\n",
            "Iteration 484/1000, x1: 0.01913675293326378, x2: 1.6536997556686401, Loss: 0.0001358935551252216\n",
            "Iteration 485/1000, x1: 0.018996799364686012, x2: 1.653704285621643, Loss: 0.00013391239917837083\n",
            "Iteration 486/1000, x1: 0.018857870250940323, x2: 1.6537086963653564, Loss: 0.00013196017243899405\n",
            "Iteration 487/1000, x1: 0.018719958141446114, x2: 1.6537129878997803, Loss: 0.00013003642379771918\n",
            "Iteration 488/1000, x1: 0.018583055585622787, x2: 1.6537173986434937, Loss: 0.00012814071669708937\n",
            "Iteration 489/1000, x1: 0.018447155132889748, x2: 1.653721570968628, Loss: 0.00012627270189113915\n",
            "Iteration 490/1000, x1: 0.018312249332666397, x2: 1.6537257432937622, Loss: 0.0001244319137185812\n",
            "Iteration 491/1000, x1: 0.01817833073437214, x2: 1.6537299156188965, Loss: 0.00012261798838153481\n",
            "Iteration 492/1000, x1: 0.018045391887426376, x2: 1.6537339687347412, Loss: 0.00012083053297828883\n",
            "Iteration 493/1000, x1: 0.017913425341248512, x2: 1.653738021850586, Loss: 0.00011906914005521685\n",
            "Iteration 494/1000, x1: 0.0177824255079031, x2: 1.6537419557571411, Loss: 0.00011733343126252294\n",
            "Iteration 495/1000, x1: 0.01765238307416439, x2: 1.6537457704544067, Loss: 0.00011562307918211445\n",
            "Iteration 496/1000, x1: 0.01752329245209694, x2: 1.6537495851516724, Loss: 0.00011393763270461932\n",
            "Iteration 497/1000, x1: 0.017395146191120148, x2: 1.653753399848938, Loss: 0.00011227677896386012\n",
            "Iteration 498/1000, x1: 0.01726793870329857, x2: 1.653757095336914, Loss: 0.00011064013233408332\n",
            "Iteration 499/1000, x1: 0.017141660675406456, x2: 1.6537607908248901, Loss: 0.00010902737994911149\n",
            "Iteration 500/1000, x1: 0.017016306519508362, x2: 1.6537643671035767, Loss: 0.00010743815073510632\n",
            "Iteration 501/1000, x1: 0.01689187064766884, x2: 1.6537679433822632, Loss: 0.00010587208089418709\n",
            "Iteration 502/1000, x1: 0.01676834560930729, x2: 1.6537715196609497, Loss: 0.0001043288575601764\n",
            "Iteration 503/1000, x1: 0.016645723953843117, x2: 1.6537749767303467, Loss: 0.00010280815331498161\n",
            "Iteration 504/1000, x1: 0.016524000093340874, x2: 1.653778314590454, Loss: 0.00010130964801646769\n",
            "Iteration 505/1000, x1: 0.016403166577219963, x2: 1.6537816524505615, Loss: 9.983298514271155e-05\n",
            "Iteration 506/1000, x1: 0.016283215954899788, x2: 1.6537848711013794, Loss: 9.837785182753578e-05\n",
            "Iteration 507/1000, x1: 0.01616414450109005, x2: 1.6537882089614868, Loss: 9.694392065284774e-05\n",
            "Iteration 508/1000, x1: 0.016045942902565002, x2: 1.6537914276123047, Loss: 9.553092968417332e-05\n",
            "Iteration 509/1000, x1: 0.015928607434034348, x2: 1.653794527053833, Loss: 9.413852239958942e-05\n",
            "Iteration 510/1000, x1: 0.01581212878227234, x2: 1.6537977457046509, Loss: 9.276642231270671e-05\n",
            "Iteration 511/1000, x1: 0.015696503221988678, x2: 1.6538007259368896, Loss: 9.141433838522062e-05\n",
            "Iteration 512/1000, x1: 0.015581723302602768, x2: 1.653803825378418, Loss: 9.008196502691135e-05\n",
            "Iteration 513/1000, x1: 0.015467783436179161, x2: 1.6538068056106567, Loss: 8.876902575138956e-05\n",
            "Iteration 514/1000, x1: 0.015354677103459835, x2: 1.6538097858428955, Loss: 8.747522224439308e-05\n",
            "Iteration 515/1000, x1: 0.015242397785186768, x2: 1.6538126468658447, Loss: 8.62002998474054e-05\n",
            "Iteration 516/1000, x1: 0.015130939893424511, x2: 1.653815507888794, Loss: 8.494394569424912e-05\n",
            "Iteration 517/1000, x1: 0.015020297840237617, x2: 1.6538183689117432, Loss: 8.370591967832297e-05\n",
            "Iteration 518/1000, x1: 0.014910465106368065, x2: 1.6538211107254028, Loss: 8.248595258919522e-05\n",
            "Iteration 519/1000, x1: 0.014801436103880405, x2: 1.6538238525390625, Loss: 8.128377521643415e-05\n",
            "Iteration 520/1000, x1: 0.014693204313516617, x2: 1.6538264751434326, Loss: 8.009913290152326e-05\n",
            "Iteration 521/1000, x1: 0.014585764147341251, x2: 1.6538292169570923, Loss: 7.893174915807322e-05\n",
            "Iteration 522/1000, x1: 0.014479110017418861, x2: 1.6538317203521729, Loss: 7.778139843139797e-05\n",
            "Iteration 523/1000, x1: 0.014373236335814, x2: 1.653834342956543, Loss: 7.66478042351082e-05\n",
            "Iteration 524/1000, x1: 0.014268137514591217, x2: 1.6538368463516235, Loss: 7.553074829047546e-05\n",
            "Iteration 525/1000, x1: 0.014163807034492493, x2: 1.653839349746704, Loss: 7.442998321494088e-05\n",
            "Iteration 526/1000, x1: 0.014060240238904953, x2: 1.6538417339324951, Loss: 7.334526890190318e-05\n",
            "Iteration 527/1000, x1: 0.013957430608570576, x2: 1.6538442373275757, Loss: 7.227636524476111e-05\n",
            "Iteration 528/1000, x1: 0.013855373486876488, x2: 1.6538466215133667, Loss: 7.122303213691339e-05\n",
            "Iteration 529/1000, x1: 0.013754062354564667, x2: 1.6538490056991577, Loss: 7.018507312750444e-05\n",
            "Iteration 530/1000, x1: 0.01365349255502224, x2: 1.6538513898849487, Loss: 6.916223355801776e-05\n",
            "Iteration 531/1000, x1: 0.013553658500313759, x2: 1.6538536548614502, Loss: 6.815431697759777e-05\n",
            "Iteration 532/1000, x1: 0.013454554602503777, x2: 1.6538559198379517, Loss: 6.716109055560082e-05\n",
            "Iteration 533/1000, x1: 0.013356175273656845, x2: 1.6538581848144531, Loss: 6.61823432892561e-05\n",
            "Iteration 534/1000, x1: 0.013258515857160091, x2: 1.653860330581665, Loss: 6.521786417579278e-05\n",
            "Iteration 535/1000, x1: 0.013161570765078068, x2: 1.653862476348877, Loss: 6.426744221244007e-05\n",
            "Iteration 536/1000, x1: 0.013065334409475327, x2: 1.6538646221160889, Loss: 6.333088094834238e-05\n",
            "Iteration 537/1000, x1: 0.012969802133738995, x2: 1.6538667678833008, Loss: 6.240796210477129e-05\n",
            "Iteration 538/1000, x1: 0.012874968349933624, x2: 1.6538687944412231, Loss: 6.149851105874404e-05\n",
            "Iteration 539/1000, x1: 0.012780828401446342, x2: 1.6538708209991455, Loss: 6.060230953153223e-05\n",
            "Iteration 540/1000, x1: 0.012687377631664276, x2: 1.6538728475570679, Loss: 5.971917198621668e-05\n",
            "Iteration 541/1000, x1: 0.012594609521329403, x2: 1.6538747549057007, Loss: 5.884891652385704e-05\n",
            "Iteration 542/1000, x1: 0.012502520345151424, x2: 1.653876781463623, Loss: 5.799133577966131e-05\n",
            "Iteration 543/1000, x1: 0.012411104515194893, x2: 1.6538786888122559, Loss: 5.714626604458317e-05\n",
            "Iteration 544/1000, x1: 0.012320357374846935, x2: 1.6538804769515991, Loss: 5.631351450574584e-05\n",
            "Iteration 545/1000, x1: 0.012230274267494678, x2: 1.653882384300232, Loss: 5.549289198825136e-05\n",
            "Iteration 546/1000, x1: 0.012140849605202675, x2: 1.6538842916488647, Loss: 5.46842347830534e-05\n",
            "Iteration 547/1000, x1: 0.012052079662680626, x2: 1.653886079788208, Loss: 5.388737190514803e-05\n",
            "Iteration 548/1000, x1: 0.011963958851993084, x2: 1.6538878679275513, Loss: 5.310211781761609e-05\n",
            "Iteration 549/1000, x1: 0.011876482516527176, x2: 1.653889536857605, Loss: 5.232831972534768e-05\n",
            "Iteration 550/1000, x1: 0.011789645999670029, x2: 1.6538913249969482, Loss: 5.156579572940245e-05\n",
            "Iteration 551/1000, x1: 0.01170344464480877, x2: 1.653892993927002, Loss: 5.081438575871289e-05\n",
            "Iteration 552/1000, x1: 0.011617873795330524, x2: 1.6538946628570557, Loss: 5.007393338019028e-05\n",
            "Iteration 553/1000, x1: 0.011532928794622421, x2: 1.6538963317871094, Loss: 4.934427488478832e-05\n",
            "Iteration 554/1000, x1: 0.011448604986071587, x2: 1.6538981199264526, Loss: 4.8625235649524257e-05\n",
            "Iteration 555/1000, x1: 0.011364897713065147, x2: 1.6538996696472168, Loss: 4.7916688345139846e-05\n",
            "Iteration 556/1000, x1: 0.01128180231899023, x2: 1.653901219367981, Loss: 4.721846926258877e-05\n",
            "Iteration 557/1000, x1: 0.011199315078556538, x2: 1.6539027690887451, Loss: 4.653042560676113e-05\n",
            "Iteration 558/1000, x1: 0.011117430403828621, x2: 1.6539043188095093, Loss: 4.585241185850464e-05\n",
            "Iteration 559/1000, x1: 0.011036144569516182, x2: 1.6539058685302734, Loss: 4.518426794675179e-05\n",
            "Iteration 560/1000, x1: 0.010955453850328922, x2: 1.653907299041748, Loss: 4.4525873818201944e-05\n",
            "Iteration 561/1000, x1: 0.010875352658331394, x2: 1.6539088487625122, Loss: 4.3877069401787594e-05\n",
            "Iteration 562/1000, x1: 0.0107958372682333, x2: 1.6539102792739868, Loss: 4.3237720092292875e-05\n",
            "Iteration 563/1000, x1: 0.010716903954744339, x2: 1.6539117097854614, Loss: 4.2607691284501925e-05\n",
            "Iteration 564/1000, x1: 0.01063854806125164, x2: 1.653913140296936, Loss: 4.198685201117769e-05\n",
            "Iteration 565/1000, x1: 0.01056076493114233, x2: 1.6539145708084106, Loss: 4.137505675316788e-05\n",
            "Iteration 566/1000, x1: 0.01048355083912611, x2: 1.6539158821105957, Loss: 4.0772181819193065e-05\n",
            "Iteration 567/1000, x1: 0.010406901128590107, x2: 1.6539173126220703, Loss: 4.017808532807976e-05\n",
            "Iteration 568/1000, x1: 0.010330812074244022, x2: 1.6539186239242554, Loss: 3.959264722652733e-05\n",
            "Iteration 569/1000, x1: 0.010255279019474983, x2: 1.6539198160171509, Loss: 3.901575837517157e-05\n",
            "Iteration 570/1000, x1: 0.010180298238992691, x2: 1.653921127319336, Loss: 3.8447258702944964e-05\n",
            "Iteration 571/1000, x1: 0.010105866007506847, x2: 1.653922438621521, Loss: 3.788704634644091e-05\n",
            "Iteration 572/1000, x1: 0.010031978599727154, x2: 1.653923749923706, Loss: 3.7334993976401165e-05\n",
            "Iteration 573/1000, x1: 0.009958631359040737, x2: 1.6539249420166016, Loss: 3.679099972941913e-05\n",
            "Iteration 574/1000, x1: 0.009885820560157299, x2: 1.653926134109497, Loss: 3.6254929000278935e-05\n",
            "Iteration 575/1000, x1: 0.009813541546463966, x2: 1.6539273262023926, Loss: 3.572667628759518e-05\n",
            "Iteration 576/1000, x1: 0.009741791523993015, x2: 1.653928518295288, Loss: 3.5206110624130815e-05\n",
            "Iteration 577/1000, x1: 0.009670565836131573, x2: 1.6539297103881836, Loss: 3.469313742243685e-05\n",
            "Iteration 578/1000, x1: 0.009599861688911915, x2: 1.653930902481079, Loss: 3.418763299123384e-05\n",
            "Iteration 579/1000, x1: 0.009529674425721169, x2: 1.653931975364685, Loss: 3.3689506381051615e-05\n",
            "Iteration 580/1000, x1: 0.009460000321269035, x2: 1.653933048248291, Loss: 3.319863753858954e-05\n",
            "Iteration 581/1000, x1: 0.009390835650265217, x2: 1.653934121131897, Loss: 3.27149209624622e-05\n",
            "Iteration 582/1000, x1: 0.009322176687419415, x2: 1.653935194015503, Loss: 3.223825478926301e-05\n",
            "Iteration 583/1000, x1: 0.00925401970744133, x2: 1.6539362668991089, Loss: 3.176852987962775e-05\n",
            "Iteration 584/1000, x1: 0.009186360985040665, x2: 1.6539373397827148, Loss: 3.1305651646107435e-05\n",
            "Iteration 585/1000, x1: 0.009119197726249695, x2: 1.6539385318756104, Loss: 3.0849510949337855e-05\n",
            "Iteration 586/1000, x1: 0.009052525274455547, x2: 1.6539394855499268, Loss: 3.0400029572774656e-05\n",
            "Iteration 587/1000, x1: 0.008986339904367924, x2: 1.6539404392242432, Loss: 2.995710019604303e-05\n",
            "Iteration 588/1000, x1: 0.0089206388220191, x2: 1.6539413928985596, Loss: 2.9520619136746973e-05\n",
            "Iteration 589/1000, x1: 0.008855418302118778, x2: 1.6539424657821655, Loss: 2.909049544541631e-05\n",
            "Iteration 590/1000, x1: 0.00879067461937666, x2: 1.653943419456482, Loss: 2.8666641810559668e-05\n",
            "Iteration 591/1000, x1: 0.008726404048502445, x2: 1.6539443731307983, Loss: 2.8248969101696275e-05\n",
            "Iteration 592/1000, x1: 0.008662603795528412, x2: 1.6539453268051147, Loss: 2.783737909339834e-05\n",
            "Iteration 593/1000, x1: 0.00859927013516426, x2: 1.6539461612701416, Loss: 2.7431789931142703e-05\n",
            "Iteration 594/1000, x1: 0.008536399342119694, x2: 1.653947114944458, Loss: 2.7032110665459186e-05\n",
            "Iteration 595/1000, x1: 0.008473988622426987, x2: 1.6539479494094849, Loss: 2.6638255803845823e-05\n",
            "Iteration 596/1000, x1: 0.008412034250795841, x2: 1.6539487838745117, Loss: 2.625014167279005e-05\n",
            "Iteration 597/1000, x1: 0.008350532501935959, x2: 1.6539497375488281, Loss: 2.586767732282169e-05\n",
            "Iteration 598/1000, x1: 0.008289480581879616, x2: 1.653950572013855, Loss: 2.5490789994364604e-05\n",
            "Iteration 599/1000, x1: 0.008228874765336514, x2: 1.6539515256881714, Loss: 2.5119390556938015e-05\n",
            "Iteration 600/1000, x1: 0.008168712258338928, x2: 1.6539522409439087, Loss: 2.4753408069955185e-05\n",
            "Iteration 601/1000, x1: 0.008108990266919136, x2: 1.6539530754089355, Loss: 2.4392751583945937e-05\n",
            "Iteration 602/1000, x1: 0.008049704134464264, x2: 1.6539539098739624, Loss: 2.403735925327055e-05\n",
            "Iteration 603/1000, x1: 0.007990851998329163, x2: 1.6539546251296997, Loss: 2.3687141947448254e-05\n",
            "Iteration 604/1000, x1: 0.007932430133223534, x2: 1.6539554595947266, Loss: 2.3342025087913498e-05\n",
            "Iteration 605/1000, x1: 0.007874435745179653, x2: 1.6539561748504639, Loss: 2.3001941372058354e-05\n",
            "Iteration 606/1000, x1: 0.007816865108907223, x2: 1.6539570093154907, Loss: 2.266681258333847e-05\n",
            "Iteration 607/1000, x1: 0.0077597154304385185, x2: 1.6539578437805176, Loss: 2.2336564143188298e-05\n",
            "Iteration 608/1000, x1: 0.007702983450144529, x2: 1.6539584398269653, Loss: 2.201113420596812e-05\n",
            "Iteration 609/1000, x1: 0.007646666374057531, x2: 1.6539591550827026, Loss: 2.1690440917154774e-05\n",
            "Iteration 610/1000, x1: 0.007590761408209801, x2: 1.65395987033844, Loss: 2.1374418793129735e-05\n",
            "Iteration 611/1000, x1: 0.007535264827311039, x2: 1.6539605855941772, Loss: 2.106300598825328e-05\n",
            "Iteration 612/1000, x1: 0.00748017430305481, x2: 1.6539613008499146, Loss: 2.0756127923959866e-05\n",
            "Iteration 613/1000, x1: 0.007425486575812101, x2: 1.6539618968963623, Loss: 2.0453726392588578e-05\n",
            "Iteration 614/1000, x1: 0.007371198385953903, x2: 1.6539626121520996, Loss: 2.015572681557387e-05\n",
            "Iteration 615/1000, x1: 0.0073173074051737785, x2: 1.653963327407837, Loss: 1.9862067347276025e-05\n",
            "Iteration 616/1000, x1: 0.007263810373842716, x2: 1.6539640426635742, Loss: 1.9572686142055318e-05\n",
            "Iteration 617/1000, x1: 0.007210704497992992, x2: 1.6539647579193115, Loss: 1.9287524992250837e-05\n",
            "Iteration 618/1000, x1: 0.007157986983656883, x2: 1.6539653539657593, Loss: 1.9006518414244056e-05\n",
            "Iteration 619/1000, x1: 0.007105655036866665, x2: 1.6539658308029175, Loss: 1.8729610019363463e-05\n",
            "Iteration 620/1000, x1: 0.007053705397993326, x2: 1.6539664268493652, Loss: 1.845673432399053e-05\n",
            "Iteration 621/1000, x1: 0.007002135738730431, x2: 1.653967022895813, Loss: 1.8187831301474944e-05\n",
            "Iteration 622/1000, x1: 0.006950943265110254, x2: 1.6539676189422607, Loss: 1.79228500201134e-05\n",
            "Iteration 623/1000, x1: 0.006900125183165073, x2: 1.6539682149887085, Loss: 1.7661724996287376e-05\n",
            "Iteration 624/1000, x1: 0.006849678698927164, x2: 1.6539688110351562, Loss: 1.7404408936272375e-05\n",
            "Iteration 625/1000, x1: 0.0067996010184288025, x2: 1.653969407081604, Loss: 1.7150843632407486e-05\n",
            "Iteration 626/1000, x1: 0.006749889347702265, x2: 1.6539700031280518, Loss: 1.6900969058042392e-05\n",
            "Iteration 627/1000, x1: 0.006700541358441114, x2: 1.6539705991744995, Loss: 1.6654734281473793e-05\n",
            "Iteration 628/1000, x1: 0.00665155379101634, x2: 1.6539711952209473, Loss: 1.6412090189987794e-05\n",
            "Iteration 629/1000, x1: 0.006602924782782793, x2: 1.653971791267395, Loss: 1.617297857592348e-05\n",
            "Iteration 630/1000, x1: 0.006554651074111462, x2: 1.6539722681045532, Loss: 1.5937353964545764e-05\n",
            "Iteration 631/1000, x1: 0.0065067303366959095, x2: 1.6539727449417114, Loss: 1.5705163605161943e-05\n",
            "Iteration 632/1000, x1: 0.0064591602422297, x2: 1.6539733409881592, Loss: 1.5476351109100506e-05\n",
            "Iteration 633/1000, x1: 0.006411937531083822, x2: 1.6539738178253174, Loss: 1.525087918707868e-05\n",
            "Iteration 634/1000, x1: 0.006365060340613127, x2: 1.6539742946624756, Loss: 1.5028687812446151e-05\n",
            "Iteration 635/1000, x1: 0.00631852587684989, x2: 1.6539747714996338, Loss: 1.4809735148446634e-05\n",
            "Iteration 636/1000, x1: 0.006272331811487675, x2: 1.653975248336792, Loss: 1.4593972991860937e-05\n",
            "Iteration 637/1000, x1: 0.006226475350558758, x2: 1.6539757251739502, Loss: 1.4381354048964567e-05\n",
            "Iteration 638/1000, x1: 0.006180954165756702, x2: 1.6539762020111084, Loss: 1.4171833754517138e-05\n",
            "Iteration 639/1000, x1: 0.006135765928775072, x2: 1.6539766788482666, Loss: 1.3965364814794157e-05\n",
            "Iteration 640/1000, x1: 0.006090907845646143, x2: 1.6539772748947144, Loss: 1.3761903574049938e-05\n",
            "Iteration 641/1000, x1: 0.006046378053724766, x2: 1.6539777517318726, Loss: 1.3561406376538798e-05\n",
            "Iteration 642/1000, x1: 0.006002173759043217, x2: 1.6539782285690308, Loss: 1.3363833204493858e-05\n",
            "Iteration 643/1000, x1: 0.005958292633295059, x2: 1.6539785861968994, Loss: 1.3169138583180029e-05\n",
            "Iteration 644/1000, x1: 0.005914732348173857, x2: 1.653978943824768, Loss: 1.2977281585335732e-05\n",
            "Iteration 645/1000, x1: 0.005871490575373173, x2: 1.6539794206619263, Loss: 1.2788216736225877e-05\n",
            "Iteration 646/1000, x1: 0.005828564986586571, x2: 1.653979778289795, Loss: 1.2601908565557096e-05\n",
            "Iteration 647/1000, x1: 0.005785953253507614, x2: 1.6539801359176636, Loss: 1.241831614606781e-05\n",
            "Iteration 648/1000, x1: 0.005743653047829866, x2: 1.6539806127548218, Loss: 1.2237395822012331e-05\n",
            "Iteration 649/1000, x1: 0.005701662041246891, x2: 1.6539809703826904, Loss: 1.2059113942086697e-05\n",
            "Iteration 650/1000, x1: 0.0056599779054522514, x2: 1.653981328010559, Loss: 1.1883428669534624e-05\n",
            "Iteration 651/1000, x1: 0.005618598777800798, x2: 1.6539818048477173, Loss: 1.1710299986589234e-05\n",
            "Iteration 652/1000, x1: 0.005577521864324808, x2: 1.653982162475586, Loss: 1.1539696970430668e-05\n",
            "Iteration 653/1000, x1: 0.005536745302379131, x2: 1.6539825201034546, Loss: 1.1371577784302644e-05\n",
            "Iteration 654/1000, x1: 0.0054962667636573315, x2: 1.6539829969406128, Loss: 1.1205906957911793e-05\n",
            "Iteration 655/1000, x1: 0.005456084385514259, x2: 1.6539833545684814, Loss: 1.1042650839954149e-05\n",
            "Iteration 656/1000, x1: 0.005416195839643478, x2: 1.65398371219635, Loss: 1.0881773050641641e-05\n",
            "Iteration 657/1000, x1: 0.005376598797738552, x2: 1.6539840698242188, Loss: 1.0723240848165005e-05\n",
            "Iteration 658/1000, x1: 0.005337291397154331, x2: 1.653984546661377, Loss: 1.0567016033746768e-05\n",
            "Iteration 659/1000, x1: 0.005298271309584379, x2: 1.6539849042892456, Loss: 1.0413069503556471e-05\n",
            "Iteration 660/1000, x1: 0.005259536672383547, x2: 1.6539852619171143, Loss: 1.0261364877806045e-05\n",
            "Iteration 661/1000, x1: 0.005221085157245398, x2: 1.6539855003356934, Loss: 1.011187214317033e-05\n",
            "Iteration 662/1000, x1: 0.005182914901524782, x2: 1.653985857963562, Loss: 9.96455673885066e-06\n",
            "Iteration 663/1000, x1: 0.005145023576915264, x2: 1.6539860963821411, Loss: 9.819387742027175e-06\n",
            "Iteration 664/1000, x1: 0.005107409320771694, x2: 1.6539864540100098, Loss: 9.67633332038531e-06\n",
            "Iteration 665/1000, x1: 0.005070069804787636, x2: 1.6539866924285889, Loss: 9.535364370094612e-06\n",
            "Iteration 666/1000, x1: 0.005033003631979227, x2: 1.653986930847168, Loss: 9.396448149345815e-06\n",
            "Iteration 667/1000, x1: 0.004996208474040031, x2: 1.6539872884750366, Loss: 9.259556463803165e-06\n",
            "Iteration 668/1000, x1: 0.004959682002663612, x2: 1.6539875268936157, Loss: 9.1246583906468e-06\n",
            "Iteration 669/1000, x1: 0.004923422820866108, x2: 1.6539878845214844, Loss: 8.991724826046266e-06\n",
            "Iteration 670/1000, x1: 0.004887428600341082, x2: 1.6539881229400635, Loss: 8.86073030414991e-06\n",
            "Iteration 671/1000, x1: 0.004851697478443384, x2: 1.6539884805679321, Loss: 8.73164117365377e-06\n",
            "Iteration 672/1000, x1: 0.004816227592527866, x2: 1.6539887189865112, Loss: 8.6044346971903e-06\n",
            "Iteration 673/1000, x1: 0.004781017079949379, x2: 1.6539890766143799, Loss: 8.479079951939639e-06\n",
            "Iteration 674/1000, x1: 0.004746064078062773, x2: 1.653989315032959, Loss: 8.355553291039541e-06\n",
            "Iteration 675/1000, x1: 0.0047113667242228985, x2: 1.6539896726608276, Loss: 8.233824701164849e-06\n",
            "Iteration 676/1000, x1: 0.00467692269012332, x2: 1.6539899110794067, Loss: 8.113871444948018e-06\n",
            "Iteration 677/1000, x1: 0.004642730578780174, x2: 1.6539902687072754, Loss: 7.995662599569187e-06\n",
            "Iteration 678/1000, x1: 0.004608788527548313, x2: 1.6539905071258545, Loss: 7.879179065639619e-06\n",
            "Iteration 679/1000, x1: 0.004575094673782587, x2: 1.6539908647537231, Loss: 7.764389920339454e-06\n",
            "Iteration 680/1000, x1: 0.004541647154837847, x2: 1.6539911031723022, Loss: 7.651275154785253e-06\n",
            "Iteration 681/1000, x1: 0.004508444108068943, x2: 1.653991460800171, Loss: 7.539807938883314e-06\n",
            "Iteration 682/1000, x1: 0.004475483670830727, x2: 1.65399169921875, Loss: 7.42996462577139e-06\n",
            "Iteration 683/1000, x1: 0.004442764446139336, x2: 1.653991937637329, Loss: 7.321721113839885e-06\n",
            "Iteration 684/1000, x1: 0.004410284571349621, x2: 1.6539920568466187, Loss: 7.215056939458009e-06\n",
            "Iteration 685/1000, x1: 0.004378042183816433, x2: 1.6539922952651978, Loss: 7.109945272532059e-06\n",
            "Iteration 686/1000, x1: 0.004346035420894623, x2: 1.6539925336837769, Loss: 7.006365649431245e-06\n",
            "Iteration 687/1000, x1: 0.004314262419939041, x2: 1.6539926528930664, Loss: 6.904295332788024e-06\n",
            "Iteration 688/1000, x1: 0.004282721783965826, x2: 1.6539928913116455, Loss: 6.803710675740149e-06\n",
            "Iteration 689/1000, x1: 0.004251411650329828, x2: 1.6539931297302246, Loss: 6.70459121465683e-06\n",
            "Iteration 690/1000, x1: 0.004220330622047186, x2: 1.6539932489395142, Loss: 6.606916940654628e-06\n",
            "Iteration 691/1000, x1: 0.00418947683647275, x2: 1.6539934873580933, Loss: 6.51066557111335e-06\n",
            "Iteration 692/1000, x1: 0.0041588484309613705, x2: 1.6539937257766724, Loss: 6.415815278160153e-06\n",
            "Iteration 693/1000, x1: 0.004128444008529186, x2: 1.653993844985962, Loss: 6.322348326648353e-06\n",
            "Iteration 694/1000, x1: 0.004098262172192335, x2: 1.653994083404541, Loss: 6.2302424339577556e-06\n",
            "Iteration 695/1000, x1: 0.004068300593644381, x2: 1.6539943218231201, Loss: 6.1394780459522735e-06\n",
            "Iteration 696/1000, x1: 0.004038558341562748, x2: 1.6539944410324097, Loss: 6.05003651799052e-06\n",
            "Iteration 697/1000, x1: 0.004009033553302288, x2: 1.6539946794509888, Loss: 5.961897386441706e-06\n",
            "Iteration 698/1000, x1: 0.003979724366217852, x2: 1.6539949178695679, Loss: 5.8750429161591455e-06\n",
            "Iteration 699/1000, x1: 0.003950629848986864, x2: 1.6539950370788574, Loss: 5.7894535530067515e-06\n",
            "Iteration 700/1000, x1: 0.003921747673302889, x2: 1.6539952754974365, Loss: 5.70511201658519e-06\n",
            "Iteration 701/1000, x1: 0.0038930769078433514, x2: 1.6539955139160156, Loss: 5.62199738851632e-06\n",
            "Iteration 702/1000, x1: 0.0038646156899631023, x2: 1.6539956331253052, Loss: 5.540095571632264e-06\n",
            "Iteration 703/1000, x1: 0.0038363623898476362, x2: 1.6539958715438843, Loss: 5.459386102302233e-06\n",
            "Iteration 704/1000, x1: 0.0038083158433437347, x2: 1.6539961099624634, Loss: 5.379851245379541e-06\n",
            "Iteration 705/1000, x1: 0.0037804741878062487, x2: 1.653996229171753, Loss: 5.3014773584436625e-06\n",
            "Iteration 706/1000, x1: 0.0037528362590819597, x2: 1.653996467590332, Loss: 5.2242430683691055e-06\n",
            "Iteration 707/1000, x1: 0.0037254001945257187, x2: 1.6539967060089111, Loss: 5.148135187482694e-06\n",
            "Iteration 708/1000, x1: 0.0036981648299843073, x2: 1.6539968252182007, Loss: 5.073135980637744e-06\n",
            "Iteration 709/1000, x1: 0.00367112853564322, x2: 1.6539970636367798, Loss: 4.999229076929623e-06\n",
            "Iteration 710/1000, x1: 0.0036442899145185947, x2: 1.6539971828460693, Loss: 4.926399924443103e-06\n",
            "Iteration 711/1000, x1: 0.0036176475696265697, x2: 1.6539974212646484, Loss: 4.854630333284149e-06\n",
            "Iteration 712/1000, x1: 0.0035911998711526394, x2: 1.6539976596832275, Loss: 4.783907115779584e-06\n",
            "Iteration 713/1000, x1: 0.0035649456549435854, x2: 1.653997778892517, Loss: 4.714214355772128e-06\n",
            "Iteration 714/1000, x1: 0.003538883291184902, x2: 1.6539980173110962, Loss: 4.645536591851851e-06\n",
            "Iteration 715/1000, x1: 0.0035130116157233715, x2: 1.6539982557296753, Loss: 4.577858817356173e-06\n",
            "Iteration 716/1000, x1: 0.0034873289987444878, x2: 1.6539983749389648, Loss: 4.5111682993592694e-06\n",
            "Iteration 717/1000, x1: 0.003461834043264389, x2: 1.653998613357544, Loss: 4.4454486669565085e-06\n",
            "Iteration 718/1000, x1: 0.003436525585129857, x2: 1.6539987325668335, Loss: 4.380686277727364e-06\n",
            "Iteration 719/1000, x1: 0.00341140222735703, x2: 1.653998851776123, Loss: 4.316867489251308e-06\n",
            "Iteration 720/1000, x1: 0.003386462340131402, x2: 1.6539989709854126, Loss: 4.253979568602517e-06\n",
            "Iteration 721/1000, x1: 0.0033617049921303988, x2: 1.6539989709854126, Loss: 4.192007509118412e-06\n",
            "Iteration 722/1000, x1: 0.0033371285535395145, x2: 1.6539990901947021, Loss: 4.130937668378465e-06\n",
            "Iteration 723/1000, x1: 0.003312731860205531, x2: 1.6539992094039917, Loss: 4.070757768204203e-06\n",
            "Iteration 724/1000, x1: 0.0032885135151445866, x2: 1.6539993286132812, Loss: 4.011455075669801e-06\n",
            "Iteration 725/1000, x1: 0.003264472121372819, x2: 1.6539994478225708, Loss: 3.953015948354732e-06\n",
            "Iteration 726/1000, x1: 0.00324060651473701, x2: 1.6539995670318604, Loss: 3.895427653333172e-06\n",
            "Iteration 727/1000, x1: 0.003216915298253298, x2: 1.6539995670318604, Loss: 3.8386788219213486e-06\n",
            "Iteration 728/1000, x1: 0.003193397307768464, x2: 1.65399968624115, Loss: 3.782756493819761e-06\n",
            "Iteration 729/1000, x1: 0.0031700513791292906, x2: 1.6539998054504395, Loss: 3.727649072970962e-06\n",
            "Iteration 730/1000, x1: 0.0031468761153519154, x2: 1.653999924659729, Loss: 3.673344281196478e-06\n",
            "Iteration 731/1000, x1: 0.00312387035228312, x2: 1.6540000438690186, Loss: 3.619830749812536e-06\n",
            "Iteration 732/1000, x1: 0.003101032692939043, x2: 1.6540000438690186, Loss: 3.5670975648827152e-06\n",
            "Iteration 733/1000, x1: 0.0030783619731664658, x2: 1.654000163078308, Loss: 3.5151317661075154e-06\n",
            "Iteration 734/1000, x1: 0.00305585702881217, x2: 1.6540002822875977, Loss: 3.4639231216715416e-06\n",
            "Iteration 735/1000, x1: 0.0030335166957229376, x2: 1.6540004014968872, Loss: 3.4134602628910216e-06\n",
            "Iteration 736/1000, x1: 0.0030113395769149065, x2: 1.6540005207061768, Loss: 3.3637329579505604e-06\n",
            "Iteration 737/1000, x1: 0.0029893245082348585, x2: 1.6540006399154663, Loss: 3.314729838166386e-06\n",
            "Iteration 738/1000, x1: 0.002967470558360219, x2: 1.6540006399154663, Loss: 3.2664408990967786e-06\n",
            "Iteration 739/1000, x1: 0.002945776330307126, x2: 1.6540007591247559, Loss: 3.2188549994316418e-06\n",
            "Iteration 740/1000, x1: 0.0029242406599223614, x2: 1.6540008783340454, Loss: 3.171962589476607e-06\n",
            "Iteration 741/1000, x1: 0.0029028623830527067, x2: 1.654000997543335, Loss: 3.1257534374162788e-06\n",
            "Iteration 742/1000, x1: 0.0028816405683755875, x2: 1.6540011167526245, Loss: 3.0802170840615872e-06\n",
            "Iteration 743/1000, x1: 0.0028605738189071417, x2: 1.654001235961914, Loss: 3.035344207091839e-06\n",
            "Iteration 744/1000, x1: 0.002839660970494151, x2: 1.654001235961914, Loss: 2.9911257115600165e-06\n",
            "Iteration 745/1000, x1: 0.002818901091814041, x2: 1.6540013551712036, Loss: 2.9475506835296983e-06\n",
            "Iteration 746/1000, x1: 0.0027982930187135935, x2: 1.6540014743804932, Loss: 2.9046104828012176e-06\n",
            "Iteration 747/1000, x1: 0.00277783558703959, x2: 1.6540015935897827, Loss: 2.8622957870538812e-06\n",
            "Iteration 748/1000, x1: 0.002757527632638812, x2: 1.6540017127990723, Loss: 2.8205979560880223e-06\n",
            "Iteration 749/1000, x1: 0.0027373682241886854, x2: 1.6540018320083618, Loss: 2.7795069854619214e-06\n",
            "Iteration 750/1000, x1: 0.0027173561975359917, x2: 1.6540018320083618, Loss: 2.739015371844289e-06\n",
            "Iteration 751/1000, x1: 0.0026974903885275126, x2: 1.6540019512176514, Loss: 2.6991131107934052e-06\n",
            "Iteration 752/1000, x1: 0.0026777698658406734, x2: 1.654002070426941, Loss: 2.65979224423063e-06\n",
            "Iteration 753/1000, x1: 0.002658193465322256, x2: 1.6540021896362305, Loss: 2.621044131956296e-06\n",
            "Iteration 754/1000, x1: 0.002638760255649686, x2: 1.65400230884552, Loss: 2.5828603611444123e-06\n",
            "Iteration 755/1000, x1: 0.0026194690726697445, x2: 1.65400230884552, Loss: 2.545233428463689e-06\n",
            "Iteration 756/1000, x1: 0.0026003189850598574, x2: 1.6540024280548096, Loss: 2.508154466340784e-06\n",
            "Iteration 757/1000, x1: 0.0025813088286668062, x2: 1.6540025472640991, Loss: 2.4716155166970566e-06\n",
            "Iteration 758/1000, x1: 0.0025624376721680164, x2: 1.6540026664733887, Loss: 2.4356086214538664e-06\n",
            "Iteration 759/1000, x1: 0.0025437045842409134, x2: 1.6540027856826782, Loss: 2.4001267320272746e-06\n",
            "Iteration 760/1000, x1: 0.002525108400732279, x2: 1.6540029048919678, Loss: 2.3651616629649652e-06\n",
            "Iteration 761/1000, x1: 0.002506648190319538, x2: 1.6540029048919678, Loss: 2.3307059109356487e-06\n",
            "Iteration 762/1000, x1: 0.002488322788849473, x2: 1.6540030241012573, Loss: 2.296752199981711e-06\n",
            "Iteration 763/1000, x1: 0.0024701314978301525, x2: 1.6540031433105469, Loss: 2.263293026771862e-06\n",
            "Iteration 764/1000, x1: 0.0024520731531083584, x2: 1.6540032625198364, Loss: 2.2303213427221635e-06\n",
            "Iteration 765/1000, x1: 0.002434146823361516, x2: 1.654003381729126, Loss: 2.197829871875001e-06\n",
            "Iteration 766/1000, x1: 0.0024163515772670507, x2: 1.6540035009384155, Loss: 2.165811565646436e-06\n",
            "Iteration 767/1000, x1: 0.002398686483502388, x2: 1.6540035009384155, Loss: 2.134260512320907e-06\n",
            "Iteration 768/1000, x1: 0.0023811503779143095, x2: 1.654003620147705, Loss: 2.103168299072422e-06\n",
            "Iteration 769/1000, x1: 0.0023637425620108843, x2: 1.6540037393569946, Loss: 2.072529468932771e-06\n",
            "Iteration 770/1000, x1: 0.0023464621044695377, x2: 1.6540038585662842, Loss: 2.0423365185706643e-06\n",
            "Iteration 771/1000, x1: 0.0023293078411370516, x2: 1.6540039777755737, Loss: 2.0125837636442157e-06\n",
            "Iteration 772/1000, x1: 0.002312279073521495, x2: 1.6540040969848633, Loss: 1.983264610316837e-06\n",
            "Iteration 773/1000, x1: 0.002295374870300293, x2: 1.6540040969848633, Loss: 1.9543726921256166e-06\n",
            "Iteration 774/1000, x1: 0.0022785940673202276, x2: 1.6540042161941528, Loss: 1.925901415233966e-06\n",
            "Iteration 775/1000, x1: 0.002261935966089368, x2: 1.6540043354034424, Loss: 1.8978445268658106e-06\n",
            "Iteration 776/1000, x1: 0.002245399635285139, x2: 1.654004454612732, Loss: 1.8701966837397777e-06\n",
            "Iteration 777/1000, x1: 0.0022289843764156103, x2: 1.6540045738220215, Loss: 1.8429512920192792e-06\n",
            "Iteration 778/1000, x1: 0.0022126890253275633, x2: 1.6540045738220215, Loss: 1.8161036905439687e-06\n",
            "Iteration 779/1000, x1: 0.002196512883529067, x2: 1.654004693031311, Loss: 1.789646603356232e-06\n",
            "Iteration 780/1000, x1: 0.002180455019697547, x2: 1.6540048122406006, Loss: 1.7635752556088846e-06\n",
            "Iteration 781/1000, x1: 0.0021645145025104284, x2: 1.6540049314498901, Loss: 1.7378835082126898e-06\n",
            "Iteration 782/1000, x1: 0.002148690400645137, x2: 1.6540050506591797, Loss: 1.7125661315731122e-06\n",
            "Iteration 783/1000, x1: 0.002132982015609741, x2: 1.6540051698684692, Loss: 1.6876172139745904e-06\n",
            "Iteration 784/1000, x1: 0.002117388416081667, x2: 1.6540051698684692, Loss: 1.6630322079436155e-06\n",
            "Iteration 785/1000, x1: 0.002101908903568983, x2: 1.6540052890777588, Loss: 1.6388049743909505e-06\n",
            "Iteration 786/1000, x1: 0.002086542546749115, x2: 1.6540054082870483, Loss: 1.6149307384694112e-06\n",
            "Iteration 787/1000, x1: 0.0020712886471301317, x2: 1.654005527496338, Loss: 1.5914044979581377e-06\n",
            "Iteration 788/1000, x1: 0.0020561462733894587, x2: 1.6540056467056274, Loss: 1.5682210232625948e-06\n",
            "Iteration 789/1000, x1: 0.002041114494204521, x2: 1.654005765914917, Loss: 1.545375312161923e-06\n",
            "Iteration 790/1000, x1: 0.0020261926110833883, x2: 1.654005765914917, Loss: 1.5228624761221e-06\n",
            "Iteration 791/1000, x1: 0.002011379925534129, x2: 1.6540058851242065, Loss: 1.5006773992354283e-06\n",
            "Iteration 792/1000, x1: 0.001996675506234169, x2: 1.654006004333496, Loss: 1.4788157614020747e-06\n",
            "Iteration 793/1000, x1: 0.0019820784218609333, x2: 1.654006004333496, Loss: 1.457272674088017e-06\n",
            "Iteration 794/1000, x1: 0.0019675882067531347, x2: 1.654006004333496, Loss: 1.4360431350723957e-06\n",
            "Iteration 795/1000, x1: 0.0019532039295881987, x2: 1.654006004333496, Loss: 1.4151231653158902e-06\n",
            "Iteration 796/1000, x1: 0.0019389247754588723, x2: 1.654006004333496, Loss: 1.3945079899713164e-06\n",
            "Iteration 797/1000, x1: 0.001924749929457903, x2: 1.654006004333496, Loss: 1.3741930615651654e-06\n",
            "Iteration 798/1000, x1: 0.0019106788095086813, x2: 1.654006004333496, Loss: 1.354173946310766e-06\n",
            "Iteration 799/1000, x1: 0.0018967104842886329, x2: 1.654006004333496, Loss: 1.334446665168798e-06\n",
            "Iteration 800/1000, x1: 0.0018828442553058267, x2: 1.654006004333496, Loss: 1.3150065569789149e-06\n",
            "Iteration 801/1000, x1: 0.0018690794240683317, x2: 1.654006004333496, Loss: 1.295849756388634e-06\n",
            "Iteration 802/1000, x1: 0.001855415292084217, x2: 1.654006004333496, Loss: 1.276971943298122e-06\n",
            "Iteration 803/1000, x1: 0.00184185104444623, x2: 1.654006004333496, Loss: 1.2583692523548962e-06\n",
            "Iteration 804/1000, x1: 0.0018283858662471175, x2: 1.654006004333496, Loss: 1.2400375908327987e-06\n",
            "Iteration 805/1000, x1: 0.0018150191754102707, x2: 1.654006004333496, Loss: 1.2219728660056717e-06\n",
            "Iteration 806/1000, x1: 0.0018017501570284367, x2: 1.654006004333496, Loss: 1.2041713262078702e-06\n",
            "Iteration 807/1000, x1: 0.0017885782290250063, x2: 1.654006004333496, Loss: 1.186628992400074e-06\n",
            "Iteration 808/1000, x1: 0.0017755025764927268, x2: 1.654006004333496, Loss: 1.1693424539771513e-06\n",
            "Iteration 809/1000, x1: 0.0017625225009396672, x2: 1.654006004333496, Loss: 1.1523076182129444e-06\n",
            "Iteration 810/1000, x1: 0.0017496373038738966, x2: 1.654006004333496, Loss: 1.1355209608154837e-06\n",
            "Iteration 811/1000, x1: 0.001736846286803484, x2: 1.654006004333496, Loss: 1.1189788438059622e-06\n",
            "Iteration 812/1000, x1: 0.0017241488676518202, x2: 1.654006004333496, Loss: 1.1026777428924106e-06\n",
            "Iteration 813/1000, x1: 0.0017115442315116525, x2: 1.654006004333496, Loss: 1.0866141337828594e-06\n",
            "Iteration 814/1000, x1: 0.0016990316798910499, x2: 1.654006004333496, Loss: 1.070784605872177e-06\n",
            "Iteration 815/1000, x1: 0.0016866106307134032, x2: 1.654006004333496, Loss: 1.0551854074947187e-06\n",
            "Iteration 816/1000, x1: 0.0016742803854867816, x2: 1.654006004333496, Loss: 1.0398135827927035e-06\n",
            "Iteration 817/1000, x1: 0.0016620403621345758, x2: 1.654006004333496, Loss: 1.024665721161e-06\n",
            "Iteration 818/1000, x1: 0.0016498897457495332, x2: 1.654006004333496, Loss: 1.0097386393681518e-06\n",
            "Iteration 819/1000, x1: 0.0016378279542550445, x2: 1.654006004333496, Loss: 9.950288131221896e-07\n",
            "Iteration 820/1000, x1: 0.0016258544055745006, x2: 1.654006004333496, Loss: 9.805334002521704e-07\n",
            "Iteration 821/1000, x1: 0.0016139684012159705, x2: 1.654006004333496, Loss: 9.662491038398002e-07\n",
            "Iteration 822/1000, x1: 0.0016021692426875234, x2: 1.654006004333496, Loss: 9.521730817141361e-07\n",
            "Iteration 823/1000, x1: 0.00159045634791255, x2: 1.654006004333496, Loss: 9.383019232700462e-07\n",
            "Iteration 824/1000, x1: 0.0015788291348144412, x2: 1.654006004333496, Loss: 9.246329000234255e-07\n",
            "Iteration 825/1000, x1: 0.001567286904901266, x2: 1.654006004333496, Loss: 9.11163056116493e-07\n",
            "Iteration 826/1000, x1: 0.0015558290760964155, x2: 1.654006004333496, Loss: 8.97889435691468e-07\n",
            "Iteration 827/1000, x1: 0.0015444549499079585, x2: 1.654006004333496, Loss: 8.848090828905697e-07\n",
            "Iteration 828/1000, x1: 0.001533163944259286, x2: 1.654006004333496, Loss: 8.719193829165306e-07\n",
            "Iteration 829/1000, x1: 0.0015219554770737886, x2: 1.654006004333496, Loss: 8.592173799115699e-07\n",
            "Iteration 830/1000, x1: 0.0015108289662748575, x2: 1.654006004333496, Loss: 8.467002885481634e-07\n",
            "Iteration 831/1000, x1: 0.0014997838297858834, x2: 1.654006004333496, Loss: 8.343656645593001e-07\n",
            "Iteration 832/1000, x1: 0.0014888194855302572, x2: 1.654006004333496, Loss: 8.222107794608746e-07\n",
            "Iteration 833/1000, x1: 0.001477935235016048, x2: 1.654006004333496, Loss: 8.102329616122006e-07\n",
            "Iteration 834/1000, x1: 0.0014671306125819683, x2: 1.654006004333496, Loss: 7.984295962160104e-07\n",
            "Iteration 835/1000, x1: 0.0014564049197360873, x2: 1.654006004333496, Loss: 7.86798239005293e-07\n",
            "Iteration 836/1000, x1: 0.0014457576908171177, x2: 1.654006004333496, Loss: 7.753363320261997e-07\n",
            "Iteration 837/1000, x1: 0.0014351882273331285, x2: 1.654006004333496, Loss: 7.640413173248817e-07\n",
            "Iteration 838/1000, x1: 0.0014246960636228323, x2: 1.654006004333496, Loss: 7.529108074777469e-07\n",
            "Iteration 839/1000, x1: 0.00141428061760962, x2: 1.654006004333496, Loss: 7.419425855914596e-07\n",
            "Iteration 840/1000, x1: 0.0014039413072168827, x2: 1.654006004333496, Loss: 7.311340368687524e-07\n",
            "Iteration 841/1000, x1: 0.001393677550368011, x2: 1.654006004333496, Loss: 7.204830012597085e-07\n",
            "Iteration 842/1000, x1: 0.0013834888814017177, x2: 1.654006004333496, Loss: 7.099870913407358e-07\n",
            "Iteration 843/1000, x1: 0.0013733747182413936, x2: 1.654006004333496, Loss: 6.996440902184986e-07\n",
            "Iteration 844/1000, x1: 0.0013633344788104296, x2: 1.654006004333496, Loss: 6.894518378430803e-07\n",
            "Iteration 845/1000, x1: 0.0013533675810322165, x2: 1.654006004333496, Loss: 6.794080036343075e-07\n",
            "Iteration 846/1000, x1: 0.0013434735592454672, x2: 1.654006004333496, Loss: 6.695104275422636e-07\n",
            "Iteration 847/1000, x1: 0.0013336519477888942, x2: 1.654006004333496, Loss: 6.597571200472885e-07\n",
            "Iteration 848/1000, x1: 0.0013239020481705666, x2: 1.654006004333496, Loss: 6.501459210994653e-07\n",
            "Iteration 849/1000, x1: 0.0013142235111445189, x2: 1.654006004333496, Loss: 6.406746706488775e-07\n",
            "Iteration 850/1000, x1: 0.00130461563821882, x2: 1.654006004333496, Loss: 6.313414360192837e-07\n",
            "Iteration 851/1000, x1: 0.0012950780801475048, x2: 1.654006004333496, Loss: 6.221441140041861e-07\n",
            "Iteration 852/1000, x1: 0.0012856102548539639, x2: 1.654006004333496, Loss: 6.130808287707623e-07\n",
            "Iteration 853/1000, x1: 0.001276211580261588, x2: 1.654006004333496, Loss: 6.041495907993522e-07\n",
            "Iteration 854/1000, x1: 0.0012668815907090902, x2: 1.654006004333496, Loss: 5.953484105702955e-07\n",
            "Iteration 855/1000, x1: 0.001257619820535183, x2: 1.654006004333496, Loss: 5.8667541225077e-07\n",
            "Iteration 856/1000, x1: 0.001248425804078579, x2: 1.654006004333496, Loss: 5.781287768513721e-07\n",
            "Iteration 857/1000, x1: 0.001239298959262669, x2: 1.654006004333496, Loss: 5.697066853826982e-07\n",
            "Iteration 858/1000, x1: 0.0012302389368414879, x2: 1.654006004333496, Loss: 5.614072051685071e-07\n",
            "Iteration 859/1000, x1: 0.0012212450383231044, x2: 1.654006004333496, Loss: 5.532288014364894e-07\n",
            "Iteration 860/1000, x1: 0.001212316914461553, x2: 1.654006004333496, Loss: 5.451693709801475e-07\n",
            "Iteration 861/1000, x1: 0.0012034540995955467, x2: 1.654006004333496, Loss: 5.37227379027172e-07\n",
            "Iteration 862/1000, x1: 0.001194656128063798, x2: 1.654006004333496, Loss: 5.294011771184159e-07\n",
            "Iteration 863/1000, x1: 0.0011859224177896976, x2: 1.654006004333496, Loss: 5.216890031078947e-07\n",
            "Iteration 864/1000, x1: 0.0011772525031119585, x2: 1.654006004333496, Loss: 5.140890948496235e-07\n",
            "Iteration 865/1000, x1: 0.001168646034784615, x2: 1.654006004333496, Loss: 5.065998607278743e-07\n",
            "Iteration 866/1000, x1: 0.0011601024307310581, x2: 1.654006004333496, Loss: 4.992198228137568e-07\n",
            "Iteration 867/1000, x1: 0.0011516213417053223, x2: 1.654006004333496, Loss: 4.919472189612861e-07\n",
            "Iteration 868/1000, x1: 0.0011432023020461202, x2: 1.654006004333496, Loss: 4.847806280849909e-07\n",
            "Iteration 869/1000, x1: 0.0011348447296768427, x2: 1.654006004333496, Loss: 4.77718458569143e-07\n",
            "Iteration 870/1000, x1: 0.0011265482753515244, x2: 1.654006004333496, Loss: 4.707590903763048e-07\n",
            "Iteration 871/1000, x1: 0.0011183124734088778, x2: 1.654006004333496, Loss: 4.6390115926442377e-07\n",
            "Iteration 872/1000, x1: 0.0011101368581876159, x2: 1.654006004333496, Loss: 4.5714307361777173e-07\n",
            "Iteration 873/1000, x1: 0.001102021080441773, x2: 1.654006004333496, Loss: 4.504834976160055e-07\n",
            "Iteration 874/1000, x1: 0.00109396455809474, x2: 1.654006004333496, Loss: 4.439209249085252e-07\n",
            "Iteration 875/1000, x1: 0.0010859669419005513, x2: 1.654006004333496, Loss: 4.374539628315688e-07\n",
            "Iteration 876/1000, x1: 0.0010780277661979198, x2: 1.654006004333496, Loss: 4.310811334562459e-07\n",
            "Iteration 877/1000, x1: 0.00107014668174088, x2: 1.654006004333496, Loss: 4.2480121464905096e-07\n",
            "Iteration 878/1000, x1: 0.0010623232228681445, x2: 1.654006004333496, Loss: 4.1861272848109365e-07\n",
            "Iteration 879/1000, x1: 0.001054556923918426, x2: 1.654006004333496, Loss: 4.1251445281886845e-07\n",
            "Iteration 880/1000, x1: 0.001046847435645759, x2: 1.654006004333496, Loss: 4.0650499499861326e-07\n",
            "Iteration 881/1000, x1: 0.0010391942923888564, x2: 1.654006004333496, Loss: 4.005830760434037e-07\n",
            "Iteration 882/1000, x1: 0.0010315971449017525, x2: 1.654006004333496, Loss: 3.947474738197343e-07\n",
            "Iteration 883/1000, x1: 0.00102405552752316, x2: 1.654006004333496, Loss: 3.889968809289712e-07\n",
            "Iteration 884/1000, x1: 0.0010165689745917916, x2: 1.654006004333496, Loss: 3.833300183941901e-07\n",
            "Iteration 885/1000, x1: 0.0010091372532770038, x2: 1.654006004333496, Loss: 3.777456925035949e-07\n",
            "Iteration 886/1000, x1: 0.0010017597815021873, x2: 1.654006004333496, Loss: 3.7224282323222724e-07\n",
            "Iteration 887/1000, x1: 0.0009944363264366984, x2: 1.654006004333496, Loss: 3.66820017916325e-07\n",
            "Iteration 888/1000, x1: 0.0009871663060039282, x2: 1.654006004333496, Loss: 3.6147625337434874e-07\n",
            "Iteration 889/1000, x1: 0.0009799494873732328, x2: 1.654006004333496, Loss: 3.562103074727929e-07\n",
            "Iteration 890/1000, x1: 0.0009727854048833251, x2: 1.654006004333496, Loss: 3.510211001866992e-07\n",
            "Iteration 891/1000, x1: 0.0009656737092882395, x2: 1.654006004333496, Loss: 3.459074662259809e-07\n",
            "Iteration 892/1000, x1: 0.0009586139931343496, x2: 1.654006004333496, Loss: 3.408683255656797e-07\n",
            "Iteration 893/1000, x1: 0.0009516059071756899, x2: 1.654006004333496, Loss: 3.3590259818083723e-07\n",
            "Iteration 894/1000, x1: 0.0009446490439586341, x2: 1.654006004333496, Loss: 3.3100923246820457e-07\n",
            "Iteration 895/1000, x1: 0.0009377430542372167, x2: 1.654006004333496, Loss: 3.2618711998111394e-07\n",
            "Iteration 896/1000, x1: 0.0009308875305578113, x2: 1.654006004333496, Loss: 3.214352943814447e-07\n",
            "Iteration 897/1000, x1: 0.0009240821236744523, x2: 1.654006004333496, Loss: 3.1675264722252905e-07\n",
            "Iteration 898/1000, x1: 0.0009173264843411744, x2: 1.654006004333496, Loss: 3.121382405879558e-07\n",
            "Iteration 899/1000, x1: 0.000910620205104351, x2: 1.654006004333496, Loss: 3.0759105129618547e-07\n",
            "Iteration 900/1000, x1: 0.0009039629949256778, x2: 1.654006004333496, Loss: 3.031101130090974e-07\n",
            "Iteration 901/1000, x1: 0.0008973544463515282, x2: 1.654006004333496, Loss: 2.9869443096686155e-07\n",
            "Iteration 902/1000, x1: 0.0008907942101359367, x2: 1.654006004333496, Loss: 2.943430956747761e-07\n",
            "Iteration 903/1000, x1: 0.000884281937032938, x2: 1.654006004333496, Loss: 2.9005516921642993e-07\n",
            "Iteration 904/1000, x1: 0.0008778172777965665, x2: 1.654006004333496, Loss: 2.8582968525370234e-07\n",
            "Iteration 905/1000, x1: 0.0008713998831808567, x2: 1.654006004333496, Loss: 2.8166579113531043e-07\n",
            "Iteration 906/1000, x1: 0.0008650294039398432, x2: 1.654006004333496, Loss: 2.7756252052313357e-07\n",
            "Iteration 907/1000, x1: 0.0008587054908275604, x2: 1.654006004333496, Loss: 2.735190491875983e-07\n",
            "Iteration 908/1000, x1: 0.000852427794598043, x2: 1.654006004333496, Loss: 2.695344676340028e-07\n",
            "Iteration 909/1000, x1: 0.0008461960242129862, x2: 1.654006004333496, Loss: 2.656079232110642e-07\n",
            "Iteration 910/1000, x1: 0.0008400097722187638, x2: 1.654006004333496, Loss: 2.617386201109184e-07\n",
            "Iteration 911/1000, x1: 0.0008338687475770712, x2: 1.654006004333496, Loss: 2.579256204171543e-07\n",
            "Iteration 912/1000, x1: 0.0008277726010419428, x2: 1.654006004333496, Loss: 2.541681851653266e-07\n",
            "Iteration 913/1000, x1: 0.0008217210415750742, x2: 1.654006004333496, Loss: 2.5046549012586183e-07\n",
            "Iteration 914/1000, x1: 0.0008157137199304998, x2: 1.654006004333496, Loss: 2.4681676791260543e-07\n",
            "Iteration 915/1000, x1: 0.0008097503450699151, x2: 1.654006004333496, Loss: 2.4322116587427445e-07\n",
            "Iteration 916/1000, x1: 0.0008038305677473545, x2: 1.654006004333496, Loss: 2.3967797346813313e-07\n",
            "Iteration 917/1000, x1: 0.0007979540387168527, x2: 1.654006004333496, Loss: 2.3618638067546271e-07\n",
            "Iteration 918/1000, x1: 0.000792120466940105, x2: 1.654006004333496, Loss: 2.3274566274267272e-07\n",
            "Iteration 919/1000, x1: 0.0007863295613788068, x2: 1.654006004333496, Loss: 2.2935503807275381e-07\n",
            "Iteration 920/1000, x1: 0.0007805809727869928, x2: 1.654006004333496, Loss: 2.2601383875553438e-07\n",
            "Iteration 921/1000, x1: 0.0007748744101263583, x2: 1.654006004333496, Loss: 2.227212974048598e-07\n",
            "Iteration 922/1000, x1: 0.0007692095823585987, x2: 1.654006004333496, Loss: 2.1947671768884902e-07\n",
            "Iteration 923/1000, x1: 0.0007635861402377486, x2: 1.654006004333496, Loss: 2.1627941748647572e-07\n",
            "Iteration 924/1000, x1: 0.0007580038509331644, x2: 1.654006004333496, Loss: 2.1312868625500414e-07\n",
            "Iteration 925/1000, x1: 0.0007524623651988804, x2: 1.654006004333496, Loss: 2.1002385608426266e-07\n",
            "Iteration 926/1000, x1: 0.0007469613919965923, x2: 1.654006004333496, Loss: 2.069642874857891e-07\n",
            "Iteration 927/1000, x1: 0.0007415006402879953, x2: 1.654006004333496, Loss: 2.0394925570599298e-07\n",
            "Iteration 928/1000, x1: 0.000736079819034785, x2: 1.654006004333496, Loss: 2.0097817809983098e-07\n",
            "Iteration 929/1000, x1: 0.0007306985789909959, x2: 1.654006004333496, Loss: 1.9805037254627678e-07\n",
            "Iteration 930/1000, x1: 0.0007253566873259842, x2: 1.654006004333496, Loss: 1.9516518534601346e-07\n",
            "Iteration 931/1000, x1: 0.0007200538530014455, x2: 1.654006004333496, Loss: 1.9232204806485242e-07\n",
            "Iteration 932/1000, x1: 0.0007147897849790752, x2: 1.654006004333496, Loss: 1.895203354251862e-07\n",
            "Iteration 933/1000, x1: 0.0007095641922205687, x2: 1.654006004333496, Loss: 1.8675942214940733e-07\n",
            "Iteration 934/1000, x1: 0.0007043768418952823, x2: 1.654006004333496, Loss: 1.8403871138161776e-07\n",
            "Iteration 935/1000, x1: 0.0006992273847572505, x2: 1.654006004333496, Loss: 1.8135769153104775e-07\n",
            "Iteration 936/1000, x1: 0.0006941155879758298, x2: 1.654006004333496, Loss: 1.7871569468752568e-07\n",
            "Iteration 937/1000, x1: 0.0006890411605127156, x2: 1.654006004333496, Loss: 1.761121950494271e-07\n",
            "Iteration 938/1000, x1: 0.0006840038113296032, x2: 1.654006004333496, Loss: 1.735466241825634e-07\n",
            "Iteration 939/1000, x1: 0.000679003307595849, x2: 1.654006004333496, Loss: 1.7101841365274595e-07\n",
            "Iteration 940/1000, x1: 0.0006740393582731485, x2: 1.654006004333496, Loss: 1.6852705186920502e-07\n",
            "Iteration 941/1000, x1: 0.0006691116723231971, x2: 1.654006004333496, Loss: 1.66071970397752e-07\n",
            "Iteration 942/1000, x1: 0.0006642200169153512, x2: 1.654006004333496, Loss: 1.6365264343676245e-07\n",
            "Iteration 943/1000, x1: 0.000659364159218967, x2: 1.654006004333496, Loss: 1.612685736063213e-07\n",
            "Iteration 944/1000, x1: 0.00065454380819574, x2: 1.654006004333496, Loss: 1.5891924931565882e-07\n",
            "Iteration 945/1000, x1: 0.0006497586728073657, x2: 1.654006004333496, Loss: 1.5660414476315054e-07\n",
            "Iteration 946/1000, x1: 0.0006450085202232003, x2: 1.654006004333496, Loss: 1.5432276256888144e-07\n",
            "Iteration 947/1000, x1: 0.0006402931176126003, x2: 1.654006004333496, Loss: 1.520746195637912e-07\n",
            "Iteration 948/1000, x1: 0.0006356121739372611, x2: 1.654006004333496, Loss: 1.498592325788195e-07\n",
            "Iteration 949/1000, x1: 0.000630965456366539, x2: 1.654006004333496, Loss: 1.476761042340513e-07\n",
            "Iteration 950/1000, x1: 0.0006263526738621294, x2: 1.654006004333496, Loss: 1.4552479399299045e-07\n",
            "Iteration 951/1000, x1: 0.0006217736518010497, x2: 1.654006004333496, Loss: 1.434048044757219e-07\n",
            "Iteration 952/1000, x1: 0.0006172280991449952, x2: 1.654006004333496, Loss: 1.413157093566042e-07\n",
            "Iteration 953/1000, x1: 0.0006127157830633223, x2: 1.654006004333496, Loss: 1.3925705388828646e-07\n",
            "Iteration 954/1000, x1: 0.0006082364125177264, x2: 1.654006004333496, Loss: 1.372283833234178e-07\n",
            "Iteration 955/1000, x1: 0.0006037898128852248, x2: 1.654006004333496, Loss: 1.3522925712550204e-07\n",
            "Iteration 956/1000, x1: 0.0005993756931275129, x2: 1.654006004333496, Loss: 1.3325926317975245e-07\n",
            "Iteration 957/1000, x1: 0.000594993878621608, x2: 1.654006004333496, Loss: 1.3131794673881814e-07\n",
            "Iteration 958/1000, x1: 0.0005906440783292055, x2: 1.654006004333496, Loss: 1.294049241096218e-07\n",
            "Iteration 959/1000, x1: 0.0005863260594196618, x2: 1.654006004333496, Loss: 1.2751976896652195e-07\n",
            "Iteration 960/1000, x1: 0.000582039647269994, x2: 1.654006004333496, Loss: 1.256620834055866e-07\n",
            "Iteration 961/1000, x1: 0.0005777845508418977, x2: 1.654006004333496, Loss: 1.23831455312029e-07\n",
            "Iteration 962/1000, x1: 0.0005735605955123901, x2: 1.654006004333496, Loss: 1.220274867819171e-07\n",
            "Iteration 963/1000, x1: 0.0005693674902431667, x2: 1.654006004333496, Loss: 1.2024982254388306e-07\n",
            "Iteration 964/1000, x1: 0.0005652050604112446, x2: 1.654006004333496, Loss: 1.1849803627228539e-07\n",
            "Iteration 965/1000, x1: 0.0005610730731859803, x2: 1.654006004333496, Loss: 1.1677178690661094e-07\n",
            "Iteration 966/1000, x1: 0.0005569712375290692, x2: 1.654006004333496, Loss: 1.1507068364835504e-07\n",
            "Iteration 967/1000, x1: 0.0005528994370251894, x2: 1.654006004333496, Loss: 1.1339433569901303e-07\n",
            "Iteration 968/1000, x1: 0.0005488573806360364, x2: 1.654006004333496, Loss: 1.1174243752520852e-07\n",
            "Iteration 969/1000, x1: 0.0005448448937386274, x2: 1.654006004333496, Loss: 1.1011458411758213e-07\n",
            "Iteration 970/1000, x1: 0.0005408617435023189, x2: 1.654006004333496, Loss: 1.0851045573190277e-07\n",
            "Iteration 971/1000, x1: 0.000536907697096467, x2: 1.654006004333496, Loss: 1.0692970420222991e-07\n",
            "Iteration 972/1000, x1: 0.0005329825798980892, x2: 1.654006004333496, Loss: 1.0537197425719569e-07\n",
            "Iteration 973/1000, x1: 0.0005290861590765417, x2: 1.654006004333496, Loss: 1.0383693904714164e-07\n",
            "Iteration 974/1000, x1: 0.0005252182018011808, x2: 1.654006004333496, Loss: 1.023242575115546e-07\n",
            "Iteration 975/1000, x1: 0.000521378533449024, x2: 1.654006004333496, Loss: 1.0083361701163085e-07\n",
            "Iteration 976/1000, x1: 0.0005175669211894274, x2: 1.654006004333496, Loss: 9.936469780313928e-08\n",
            "Iteration 977/1000, x1: 0.0005137831903994083, x2: 1.654006004333496, Loss: 9.791717303642145e-08\n",
            "Iteration 978/1000, x1: 0.0005100271082483232, x2: 1.654006004333496, Loss: 9.649073007267361e-08\n",
            "Iteration 979/1000, x1: 0.0005062985001131892, x2: 1.654006004333496, Loss: 9.508507048394677e-08\n",
            "Iteration 980/1000, x1: 0.0005025971331633627, x2: 1.654006004333496, Loss: 9.369990294771924e-08\n",
            "Iteration 981/1000, x1: 0.000498922832775861, x2: 1.654006004333496, Loss: 9.233489350890522e-08\n",
            "Iteration 982/1000, x1: 0.0004952753661200404, x2: 1.654006004333496, Loss: 9.098977216126514e-08\n",
            "Iteration 983/1000, x1: 0.0004916545585729182, x2: 1.654006004333496, Loss: 8.966424047684995e-08\n",
            "Iteration 984/1000, x1: 0.00048806023551151156, x2: 1.654006004333496, Loss: 8.835801423856537e-08\n",
            "Iteration 985/1000, x1: 0.0004844921932090074, x2: 1.654006004333496, Loss: 8.707083054559916e-08\n",
            "Iteration 986/1000, x1: 0.00048095022793859243, x2: 1.654006004333496, Loss: 8.580239097000231e-08\n",
            "Iteration 987/1000, x1: 0.000477434165077284, x2: 1.654006004333496, Loss: 8.455243261096257e-08\n",
            "Iteration 988/1000, x1: 0.0004739438008982688, x2: 1.654006004333496, Loss: 8.332068546224036e-08\n",
            "Iteration 989/1000, x1: 0.0004704789607785642, x2: 1.654006004333496, Loss: 8.210687241216874e-08\n",
            "Iteration 990/1000, x1: 0.00046703944099135697, x2: 1.654006004333496, Loss: 8.091075187621755e-08\n",
            "Iteration 991/1000, x1: 0.00046362506691366434, x2: 1.654006004333496, Loss: 7.973205384814719e-08\n",
            "Iteration 992/1000, x1: 0.0004602356639225036, x2: 1.654006004333496, Loss: 7.85705225325728e-08\n",
            "Iteration 993/1000, x1: 0.000456871057394892, x2: 1.654006004333496, Loss: 7.742591634496421e-08\n",
            "Iteration 994/1000, x1: 0.0004535310436040163, x2: 1.654006004333496, Loss: 7.629799370079127e-08\n",
            "Iteration 995/1000, x1: 0.00045021544792689383, x2: 1.654006004333496, Loss: 7.51864988046691e-08\n",
            "Iteration 996/1000, x1: 0.0004469240957405418, x2: 1.654006004333496, Loss: 7.40911971774949e-08\n",
            "Iteration 997/1000, x1: 0.00044365678331814706, x2: 1.654006004333496, Loss: 7.301185434016588e-08\n",
            "Iteration 998/1000, x1: 0.0004404133651405573, x2: 1.654006004333496, Loss: 7.194822160272452e-08\n",
            "Iteration 999/1000, x1: 0.00043719366658478975, x2: 1.654006004333496, Loss: 7.090009290777743e-08\n",
            "Iteration 1000/1000, x1: 0.00043399748392403126, x2: 1.654006004333496, Loss: 6.986723377622184e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = torch.exp(z)\n",
        "final_result = (x1.item(), x2.item())\n",
        "print(f'Optimal solution: x1 = {final_result[0]}, x2 = {final_result[1]}, Minimum value: {objective(*final_result)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl8oEOeUSZHX",
        "outputId": "3ac8ee2a-dc57-4a54-bf39-490367f7f241"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution: x1 = 0.00043399748392403126, x2 = 1.654006004333496, Minimum value: 6.884941585300933e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Classify MNIST Dataset with MyGD Optimizer"
      ],
      "metadata": {
        "id": "YS7wMHgQSeSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "hezp4gcmS05u"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load MNIST dataset"
      ],
      "metadata": {
        "id": "Hos0a6zUTCp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "HLSzcMXgTIwP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Network"
      ],
      "metadata": {
        "id": "Kv_sD-q2S2Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MYXp1LOpS8mO"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyNet()\n",
        "optimizer = MyGD(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "21tI0i20MOHS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "kQkpZH1qTTqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values = []\n",
        "\n",
        "for epoch in range(20):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        loss_values.append(loss.item())\n",
        "\n",
        "        # Print the loss\n",
        "    print(f'Epoch [{epoch + 1}/{20}], Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "YMzxMWVCTV80",
        "outputId": "3518113e-e713-4e24-e0c3-c2c7e797a48c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 2.067448616027832\n",
            "Epoch [2/20], Loss: 1.6720539331436157\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-00e2f4c28ac0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"std evaluated to zero after conversion to {dtype}, leading to division by zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.errorbar(range(1, len(loss_values) + 1), loss_values, fmt='-', yerr=None, ecolor='red', capsize=5)\n",
        "plt.title('Training Loss with Error Bars')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OWLyCUvDZRY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model"
      ],
      "metadata": {
        "id": "MsBtDncHTh38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {100 * accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "ku9rlWqiTjpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "wPaVF04YT20f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function\n",
        "def objective_function(x):\n",
        "    return np.log(np.exp(x[0]) + np.exp(x[1]))\n",
        "\n",
        "# Define the gradient of the objective function\n",
        "def gradient(x):\n",
        "    exp_term = np.exp(x)\n",
        "    return np.array([exp_term[0] / (exp_term[0] + exp_term[1]), exp_term[1] / (exp_term[0] + exp_term[1])])\n",
        "\n",
        "# Backtracking Line Search Algorithm\n",
        "def backtracking_line_search(x, p, alpha_bar, rho, c):\n",
        "    alpha = alpha_bar\n",
        "    while objective_function(x + alpha * p) > objective_function(x) + c * alpha * np.dot(gradient(x), p):\n",
        "        alpha *= rho\n",
        "    return alpha\n",
        "\n",
        "def trust_region(x, radius):\n",
        "    result = minimize(objective_function, x, method='trust-constr', jac=gradient,\n",
        "                      options={'disp': False, 'xtol': 1e-8, 'gtol': 1e-8})\n",
        "    return result.x\n",
        "\n",
        "# Plot Contour Lines of the Objective Function\n",
        "def plot_contour():\n",
        "    x1 = np.linspace(-2, 2, 400)\n",
        "    x2 = np.linspace(-2, 2, 400)\n",
        "    X1, X2 = np.meshgrid(x1, x2)\n",
        "    Z = np.log(np.exp(X1) + np.exp(X2))\n",
        "    plt.contour(X1, X2, Z, levels=20, cmap='viridis')\n",
        "    plt.xlabel('x1')\n",
        "    plt.ylabel('x2')\n",
        "    plt.title('Contour Plot of Objective Function')\n",
        "\n",
        "# Main Optimization Loop\n",
        "def optimize(starting_point):\n",
        "    x_k = starting_point\n",
        "    alpha_bar = 1.0\n",
        "    rho = 0.5\n",
        "    c = 0.1\n",
        "    trust_region_radius = 1.0\n",
        "\n",
        "    path_backtracking = [x_k.copy()]\n",
        "    path_trust_region = [x_k.copy()]\n",
        "\n",
        "    while True:\n",
        "        gradient_k = gradient(x_k)\n",
        "        p_k = -gradient_k  # Newton direction\n",
        "\n",
        "        # Backtracking Line Search\n",
        "        alpha_k = backtracking_line_search(x_k, p_k, alpha_bar, rho, c)\n",
        "        x_k += alpha_k * p_k\n",
        "        path_backtracking.append(x_k.copy())\n",
        "\n",
        "        # Trust Region with Dogleg Method\n",
        "        x_k = trust_region(x_k, trust_region_radius)\n",
        "        path_trust_region.append(x_k.copy())\n",
        "\n",
        "        # Check convergence criterion (you may need to define your own convergence criteria)\n",
        "        if np.linalg.norm(gradient_k) < 1e-6:\n",
        "            break\n",
        "\n",
        "    return path_backtracking, path_trust_region\n",
        "\n",
        "# Run the optimization\n",
        "starting_point = np.array([1.2, 1.2])\n",
        "path_backtracking, path_trust_region = optimize(starting_point)\n",
        "\n",
        "# Plot the results\n",
        "plot_contour()\n",
        "path_backtracking = np.array(path_backtracking)\n",
        "path_trust_region = np.array(path_trust_region)\n",
        "plt.plot(path_backtracking[:, 0], path_backtracking[:, 1], marker='o', label='Backtracking Line Search')\n",
        "plt.plot(path_trust_region[:, 0], path_trust_region[:, 1], marker='s', label='Trust Region (Dogleg Method)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9lPfVJE6XCZo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eIWG5YSB148H",
        "aQRGn10ph9-0",
        "WACv3pIlh3iK",
        "v1bWZVoglf-R",
        "K9TKn6_yK9w4",
        "AELgfZLcK9w_",
        "seNeIGQy2Yuf",
        "3MarQBS14uTt",
        "34m7iWJk4uT0",
        "XIAVzx9_7-WC",
        "WzYr_O-C9Anh"
      ],
      "authorship_tag": "ABX9TyN51C2OJuoUzdvWDAa/PlkY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}